{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Zero Full 3-Iteration Training (Google Colab)\n",
    "\n",
    "**Complete Implementation** of the Dr. Zero paper (arXiv:2601.07055)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the **full 3-iteration self-evolution pipeline**:\n",
    "- **HRPO** (Hop-grouped Relative Policy Optimization) for proposer training\n",
    "- **GRPO** for solver training\n",
    "- **Difficulty-guided reward** (Paper Equation 4)\n",
    "- **4:3:2:1 hop ratio** for question distribution\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab Pro+ (A100 80GB GPU)\n",
    "- 2TB Google Drive storage\n",
    "- ~28-42 hours total runtime (can be split across sessions)\n",
    "\n",
    "## Paper Fidelity\n",
    "\n",
    "| Parameter | Paper | This Implementation |\n",
    "|-----------|-------|--------------------|\n",
    "| Algorithm (Proposer) | HRPO | HRPO |\n",
    "| Algorithm (Solver) | GRPO | GRPO |\n",
    "| Steps/Iteration | 50 | 50 |\n",
    "| Iterations | 3 | 3 |\n",
    "| Hop Ratio | 4:3:2:1 | 4:3:2:1 |\n",
    "| Reward Rollout N | 5 | 5 |\n",
    "| Effective Batch | 256 | 256 (32×8) |\n",
    "\n",
    "**Only hardware parameters adapted** (8 GPU → 1 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Mount Google Drive & Setup Directories\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Mount Google Drive (2TB)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Directory structure for full 3-iteration training\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_full')\n",
    "\n",
    "# Persistent storage (Google Drive)\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus'\n",
    "DATA_DIR = DRIVE_BASE / 'data'\n",
    "CHECKPOINTS_DIR = DRIVE_BASE / 'checkpoints'\n",
    "LOGS_DIR = DRIVE_BASE / 'logs'\n",
    "\n",
    "# Create iteration-specific checkpoint directories\n",
    "for i in range(1, 4):\n",
    "    (CHECKPOINTS_DIR / f'iter{i}' / 'proposer').mkdir(parents=True, exist_ok=True)\n",
    "    (CHECKPOINTS_DIR / f'iter{i}' / 'solver').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create all directories\n",
    "for d in [CORPUS_DIR, DATA_DIR, LOGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Local temp storage (faster I/O)\n",
    "LOCAL_BASE = Path('/tmp/drzero')\n",
    "LOCAL_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Storage Layout:\")\n",
    "print(f\"  Google Drive: {DRIVE_BASE}\")\n",
    "print(f\"    - Corpus: {CORPUS_DIR}\")\n",
    "print(f\"    - Data: {DATA_DIR}\")\n",
    "print(f\"    - Checkpoints: {CHECKPOINTS_DIR}\")\n",
    "print(f\"  Local temp: {LOCAL_BASE}\")\n",
    "print(\"\\n[OK] Directories created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "# IMPORTANT: After this cell, restart runtime and skip to Cell 3\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installing dependencies (10-15 minutes)...\")\n",
    "\n",
    "# Core packages\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy<2.0\"])\n",
    "\n",
    "print(\"1/5 Installing ML packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"torch\", \"transformers\", \"accelerate\", \"datasets\", \"sentence-transformers\"])\n",
    "\n",
    "print(\"2/5 Installing utilities...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"biopython\", \"wandb\", \"tqdm\", \"psutil\", \"faiss-gpu\", \"uvicorn\", \n",
    "    \"fastapi\", \"pydantic\", \"pandas\", \"pyarrow\", \"httpx\", \"openai\"])\n",
    "\n",
    "print(\"3/5 Installing SGLang...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sglang[all]\"])\n",
    "\n",
    "print(\"4/5 Installing veRL...\")\n",
    "import os\n",
    "if not os.path.exists('/content/verl'):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"-q\", \n",
    "        \"https://github.com/volcengine/verl.git\", \"/content/verl\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/verl\"])\n",
    "\n",
    "print(\"5/5 Installing DrPubMedZero...\")\n",
    "if not os.path.exists('/content/DrPubMedZero'):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"-q\",\n",
    "        \"https://github.com/ShivaAyyar/DrPubMedZero.git\", \"/content/DrPubMedZero\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/DrPubMedZero\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESTART RUNTIME NOW: Runtime -> Restart session\")\n",
    "print(\"Then run Cell 1 again, skip Cell 2, continue from Cell 3\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Verify Setup & Clone Repository\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Verifying installations...\")\n",
    "\n",
    "import numpy as np\n",
    "print(f\"  [OK] numpy {np.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"  [OK] torch {torch.__version__}\")\n",
    "assert torch.cuda.is_available(), \"ERROR: No GPU!\"\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"  [OK] GPU: {gpu_name} ({gpu_mem:.0f}GB)\")\n",
    "assert gpu_mem >= 70, \"ERROR: Need A100 80GB, got smaller GPU\"\n",
    "\n",
    "import transformers\n",
    "print(f\"  [OK] transformers {transformers.__version__}\")\n",
    "\n",
    "import faiss\n",
    "print(f\"  [OK] faiss\")\n",
    "\n",
    "# Clone/update repository\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "if not REPO_DIR.exists():\n",
    "    subprocess.check_call([\"git\", \"clone\",\n",
    "        \"https://github.com/ShivaAyyar/DrPubMedZero.git\", str(REPO_DIR)])\n",
    "else:\n",
    "    subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], \n",
    "                   capture_output=True)\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "# Verify custom modules\n",
    "from biomedical import PubMedCorpusManager\n",
    "print(\"  [OK] biomedical module\")\n",
    "\n",
    "from verl.custom_reward.dr_zero_reward import compute_difficulty_reward\n",
    "print(\"  [OK] dr_zero_reward module\")\n",
    "\n",
    "from verl.custom_reward.hrpo_advantage import compute_hrpo_advantages\n",
    "print(\"  [OK] hrpo_advantage module\")\n",
    "\n",
    "from verl.custom_reward.hop_counter import count_hops\n",
    "print(\"  [OK] hop_counter module\")\n",
    "\n",
    "print(f\"\\n[OK] All checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration for Full 3-Iteration Training\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "# Re-establish paths\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_full')\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus'\n",
    "DATA_DIR = DRIVE_BASE / 'data'\n",
    "CHECKPOINTS_DIR = DRIVE_BASE / 'checkpoints'\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    \n",
    "    # Corpus - larger for full training\n",
    "    'corpus_size': 200000,  # 200K papers\n",
    "    'training_seeds': 5000,\n",
    "    'pubmed_query': '(cancer OR diabetes OR alzheimer OR cardiovascular) AND (gene OR protein OR mutation)',\n",
    "    'date_range': ('2020/01/01', '2024/12/31'),\n",
    "    \n",
    "    # Training parameters (from Paper Tables 5-6)\n",
    "    'micro_batch_size': 32,\n",
    "    'gradient_accumulation': 8,  # 32 * 8 = 256 effective batch\n",
    "    'proposer_lr': 1e-6,\n",
    "    'solver_lr': 1e-6,\n",
    "    'steps_per_iteration': 50,\n",
    "    'num_iterations': 3,\n",
    "    'save_freq': 25,\n",
    "    \n",
    "    # HRPO/GRPO parameters (from Paper)\n",
    "    'proposer_group_size': 1,\n",
    "    'solver_group_size': 5,\n",
    "    'reward_rollout_n': 5,\n",
    "    'proposer_kl_coef': 0.0,  # Paper: KL=0 for HRPO\n",
    "    'solver_kl_coef': 0.001,\n",
    "    'solver_clip_ratio': 0.2,\n",
    "    'hop_ratio': {1: 4, 2: 3, 3: 2, 4: 1},  # Paper default\n",
    "    \n",
    "    # Sequence lengths (from Paper)\n",
    "    'proposer_max_seq_len': 4096,\n",
    "    'solver_max_seq_len': 3072,\n",
    "    'max_turns': 5,\n",
    "    \n",
    "    # Paths\n",
    "    'corpus_file': str(CORPUS_DIR / 'pubmed-corpus.jsonl'),\n",
    "    'index_file': str(CORPUS_DIR / 'pubmedbert_index.faiss'),\n",
    "    'train_parquet': str(DATA_DIR / 'training_seeds.parquet'),\n",
    "    'repo_dir': str(REPO_DIR),\n",
    "    'checkpoints_dir': str(CHECKPOINTS_DIR),\n",
    "    \n",
    "    # Server ports\n",
    "    'retrieval_port': 8000,\n",
    "    'solver_port': 8001,\n",
    "}\n",
    "\n",
    "# User email for NCBI\n",
    "CONFIG['email'] = input(\"Enter email for NCBI API: \")\n",
    "\n",
    "# W&B (optional)\n",
    "wandb_key = getpass.getpass(\"W&B API key (Enter to skip): \")\n",
    "if wandb_key.strip():\n",
    "    os.environ['WANDB_API_KEY'] = wandb_key\n",
    "    import wandb\n",
    "    wandb.login(key=wandb_key)\n",
    "    print(\"  [OK] W&B configured\")\n",
    "else:\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "    print(\"  [OK] W&B disabled\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Corpus: {CONFIG['corpus_size']:,} papers\")\n",
    "print(f\"  Seeds: {CONFIG['training_seeds']:,}\")\n",
    "print(f\"  Iterations: {CONFIG['num_iterations']}\")\n",
    "print(f\"  Steps/iteration: {CONFIG['steps_per_iteration']}\")\n",
    "print(f\"  Effective batch: {CONFIG['micro_batch_size'] * CONFIG['gradient_accumulation']}\")\n",
    "print(f\"  Hop ratio: {CONFIG['hop_ratio']}\")\n",
    "print(f\"\\n[OK] Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Download PubMed Corpus (200K papers)\n",
    "\n",
    "from biomedical import PubMedCorpusManager\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "\n",
    "if corpus_file.exists():\n",
    "    with open(corpus_file) as f:\n",
    "        n_papers = sum(1 for _ in f)\n",
    "    print(f\"Corpus exists: {n_papers:,} papers\")\n",
    "    if n_papers >= CONFIG['corpus_size'] * 0.9:  # 90% threshold\n",
    "        print(\"Skipping download.\")\n",
    "        download_needed = False\n",
    "    else:\n",
    "        download_needed = True\n",
    "else:\n",
    "    download_needed = True\n",
    "\n",
    "if download_needed:\n",
    "    print(f\"Downloading {CONFIG['corpus_size']:,} PubMed papers...\")\n",
    "    print(\"Estimated time: 2-4 hours\\n\")\n",
    "    \n",
    "    manager = PubMedCorpusManager(\n",
    "        save_path=str(CORPUS_DIR),\n",
    "        email=CONFIG['email']\n",
    "    )\n",
    "    \n",
    "    articles = manager.download_pubmed_abstracts(\n",
    "        query=CONFIG['pubmed_query'],\n",
    "        max_results=CONFIG['corpus_size'],\n",
    "        date_range=CONFIG['date_range']\n",
    "    )\n",
    "    \n",
    "    if articles:\n",
    "        manager.save_corpus(articles)\n",
    "        print(f\"\\n[OK] Downloaded {len(articles):,} papers!\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Download failed\")\n",
    "else:\n",
    "    print(\"[OK] Using existing corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Build PubMedBERT FAISS Index\n",
    "\n",
    "from biomedical.biomedical_retriever import BiomedicalRetrieverServer\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "index_file = Path(CONFIG['index_file'])\n",
    "\n",
    "if index_file.exists():\n",
    "    print(f\"Index exists: {index_file}\")\n",
    "    print(\"Skipping build.\")\n",
    "else:\n",
    "    print(\"Building PubMedBERT FAISS index...\")\n",
    "    print(\"Estimated time: 1-2 hours for 200K papers\\n\")\n",
    "    \n",
    "    retriever = BiomedicalRetrieverServer(\n",
    "        corpus_path=str(corpus_file),\n",
    "        index_path=str(index_file),\n",
    "        model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        device=\"cuda\",\n",
    "        topk=3\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    results = retriever.search(\"BRCA1 mutation breast cancer\")\n",
    "    print(f\"Test search: {len(results)} results\")\n",
    "    \n",
    "    del retriever\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    size_gb = os.path.getsize(index_file) / 1e9\n",
    "    print(f\"\\n[OK] Index built: {size_gb:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Prepare Training Seeds with Hop Distribution\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from verl.custom_reward.hop_counter import generate_hop_distribution, assign_hop_to_seeds\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "parquet_file = Path(CONFIG['train_parquet'])\n",
    "\n",
    "if parquet_file.exists():\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    print(f\"Training data exists: {len(df)} examples\")\n",
    "    \n",
    "    # Check hop distribution\n",
    "    if 'data_source' in df.columns:\n",
    "        hops = df['data_source'].apply(lambda x: int(x.split('_')[-1]) if '_' in x else 1)\n",
    "        print(f\"Hop distribution: {hops.value_counts().sort_index().to_dict()}\")\n",
    "else:\n",
    "    print(f\"Preparing {CONFIG['training_seeds']} training seeds...\")\n",
    "    print(f\"Target hop ratio: {CONFIG['hop_ratio']}\")\n",
    "    \n",
    "    # Load corpus\n",
    "    corpus = []\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    print(f\"  Loaded {len(corpus):,} papers\")\n",
    "    \n",
    "    # Filter for substantial abstracts\n",
    "    substantial = [p for p in corpus if len(p.get('abstract', '').split()) > 100]\n",
    "    print(f\"  {len(substantial):,} have >100 word abstracts\")\n",
    "    \n",
    "    # Sample seeds\n",
    "    import random\n",
    "    source = substantial if len(substantial) >= CONFIG['training_seeds'] else corpus\n",
    "    seeds = random.sample(source, min(CONFIG['training_seeds'], len(source)))\n",
    "    \n",
    "    # Assign hop counts following 4:3:2:1 ratio\n",
    "    hop_counts = generate_hop_distribution(len(seeds), CONFIG['hop_ratio'])\n",
    "    \n",
    "    # Convert to veRL format\n",
    "    data = []\n",
    "    for idx, (seed, hop) in enumerate(zip(seeds, hop_counts)):\n",
    "        title = seed.get('title', '')\n",
    "        abstract = seed.get('abstract', seed.get('text', ''))[:800]\n",
    "        pmid = seed.get('pmid', str(idx))\n",
    "        \n",
    "        user_msg = f\"Generate a challenging {hop}-hop biomedical question from this paper:\\n\\nTitle: {title}\\n\\nAbstract: {abstract}\"\n",
    "        \n",
    "        data.append({\n",
    "            'prompt': [{'role': 'user', 'content': user_msg}],\n",
    "            'data_source': f'search_biomedical_{hop}',\n",
    "            'extra_info': {\n",
    "                'index': idx,\n",
    "                'pmid': pmid,\n",
    "                'hop': hop,\n",
    "                'tools_kwargs': {},\n",
    "                'interaction_kwargs': {}\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_parquet(parquet_file)\n",
    "    \n",
    "    # Verify hop distribution\n",
    "    from collections import Counter\n",
    "    hop_dist = Counter(hop_counts)\n",
    "    print(f\"\\nHop distribution:\")\n",
    "    for h in sorted(hop_dist.keys()):\n",
    "        pct = hop_dist[h] / len(hop_counts) * 100\n",
    "        print(f\"  {h}-hop: {hop_dist[h]} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n[OK] Saved {len(df)} training seeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Pre-Training Verification\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRE-TRAINING VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. GPU\n",
    "print(\"\\n1. GPU Status:\")\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"   {gpu_name}: {gpu_mem:.0f}GB\")\n",
    "assert gpu_mem >= 70, \"Need A100 80GB\"\n",
    "print(\"   [OK]\")\n",
    "\n",
    "# 2. Files\n",
    "print(\"\\n2. Required Files:\")\n",
    "files = [\n",
    "    ('Corpus', CONFIG['corpus_file']),\n",
    "    ('Index', CONFIG['index_file']),\n",
    "    ('Training data', CONFIG['train_parquet']),\n",
    "]\n",
    "all_ok = True\n",
    "for name, path in files:\n",
    "    exists = Path(path).exists()\n",
    "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
    "    print(f\"   {status} {name}\")\n",
    "    all_ok = all_ok and exists\n",
    "\n",
    "# 3. Training data\n",
    "print(\"\\n3. Training Data:\")\n",
    "df = pd.read_parquet(CONFIG['train_parquet'])\n",
    "print(f\"   Examples: {len(df)}\")\n",
    "hops = df['data_source'].apply(lambda x: int(x.split('_')[-1]))\n",
    "print(f\"   Hop distribution: {hops.value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# 4. Custom modules\n",
    "print(\"\\n4. Custom Modules:\")\n",
    "try:\n",
    "    from verl.custom_reward.dr_zero_reward import compute_difficulty_reward, compute_format_reward\n",
    "    print(\"   [OK] dr_zero_reward\")\n",
    "except ImportError as e:\n",
    "    print(f\"   [FAIL] dr_zero_reward: {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "try:\n",
    "    from verl.custom_reward.hrpo_advantage import compute_hrpo_advantages, HRPOAdvantageEstimator\n",
    "    print(\"   [OK] hrpo_advantage\")\n",
    "except ImportError as e:\n",
    "    print(f\"   [FAIL] hrpo_advantage: {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "try:\n",
    "    from verl.custom_reward.hop_counter import count_hops, HopCounter\n",
    "    print(\"   [OK] hop_counter\")\n",
    "except ImportError as e:\n",
    "    print(f\"   [FAIL] hop_counter: {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_ok:\n",
    "    print(\"ALL CHECKS PASSED - Ready for training!\")\n",
    "else:\n",
    "    print(\"SOME CHECKS FAILED - Fix issues above\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Launch Retrieval Server\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LAUNCHING RETRIEVAL SERVER (Port 8000)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "\n",
    "retrieval_cmd = [\n",
    "    \"python\", str(REPO_DIR / \"search\" / \"retrieval_server.py\"),\n",
    "    \"--mode=biomedical\",\n",
    "    f\"--index_path={CONFIG['index_file']}\",\n",
    "    f\"--corpus_path={CONFIG['corpus_file']}\",\n",
    "    \"--retriever_model=microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    \"--faiss_gpu\",\n",
    "    \"--topk=3\",\n",
    "    f\"--port={CONFIG['retrieval_port']}\"\n",
    "]\n",
    "\n",
    "# Start server\n",
    "log_file = open('/tmp/retrieval_server.log', 'w')\n",
    "retrieval_proc = subprocess.Popen(\n",
    "    retrieval_cmd,\n",
    "    stdout=log_file,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    cwd=str(REPO_DIR)\n",
    ")\n",
    "CONFIG['retrieval_pid'] = retrieval_proc.pid\n",
    "CONFIG['retrieval_log'] = log_file\n",
    "\n",
    "print(f\"Started retrieval server (PID: {retrieval_proc.pid})\")\n",
    "print(\"Waiting for server to be ready...\")\n",
    "\n",
    "# Wait for server\n",
    "for i in range(120):  # 2 minutes timeout\n",
    "    try:\n",
    "        resp = requests.get(f\"http://127.0.0.1:{CONFIG['retrieval_port']}/health\", timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            print(f\"\\n[OK] Retrieval server ready! ({i+1}s)\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "    if i % 10 == 9:\n",
    "        print(f\"  Still waiting... ({i+1}s)\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] Server may not be ready. Check logs:\")\n",
    "    print(\"  !tail -50 /tmp/retrieval_server.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Launch Solver Server (Base Model for Iteration 1)\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LAUNCHING SOLVER SERVER (Port 8001)\")\n",
    "print(\"Using base model for Iteration 1\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For iteration 1, use base model\n",
    "solver_model = CONFIG['model_name']\n",
    "\n",
    "solver_cmd = [\n",
    "    \"python\", \"-m\", \"sglang.launch_server\",\n",
    "    f\"--model-path={solver_model}\",\n",
    "    f\"--port={CONFIG['solver_port']}\",\n",
    "    \"--tool-call-parser=qwen25\",\n",
    "    \"--mem-fraction-static=0.35\",\n",
    "    \"--tp-size=1\",\n",
    "    \"--dp-size=1\",\n",
    "]\n",
    "\n",
    "log_file = open('/tmp/solver_server.log', 'w')\n",
    "solver_proc = subprocess.Popen(\n",
    "    solver_cmd,\n",
    "    stdout=log_file,\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "CONFIG['solver_pid'] = solver_proc.pid\n",
    "CONFIG['solver_log'] = log_file\n",
    "CONFIG['current_solver_model'] = solver_model\n",
    "\n",
    "print(f\"Started solver server (PID: {solver_proc.pid})\")\n",
    "print(f\"Model: {solver_model}\")\n",
    "print(\"Waiting for server to be ready (may take 3-5 minutes)...\")\n",
    "\n",
    "for i in range(300):  # 5 minutes timeout\n",
    "    try:\n",
    "        resp = requests.get(f\"http://127.0.0.1:{CONFIG['solver_port']}/health\", timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            print(f\"\\n[OK] Solver server ready! ({i+1}s)\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "    if i % 30 == 29:\n",
    "        print(f\"  Still waiting... ({i+1}s)\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] Server may not be ready. Check logs:\")\n",
    "    print(\"  !tail -50 /tmp/solver_server.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Train Proposer Iteration 1 (HRPO)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "ITERATION = 1\n",
    "checkpoint_dir = Path(CONFIG['checkpoints_dir']) / f'iter{ITERATION}' / 'proposer'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ITERATION {ITERATION}: PROPOSER TRAINING (HRPO)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAlgorithm: HRPO (Hop-grouped RPO)\")\n",
    "print(f\"  - NO ratio clipping (strictly on-policy)\")\n",
    "print(f\"  - KL coefficient: {CONFIG['proposer_kl_coef']}\")\n",
    "print(f\"  - Group size: {CONFIG['proposer_group_size']}\")\n",
    "print(f\"  - Reward rollouts: {CONFIG['reward_rollout_n']}\")\n",
    "print(f\"\\nReward: Difficulty-guided (Paper Eq. 4)\")\n",
    "print(f\"  r = I(0<k<n) × (n-k)/(n-1) + r_format\")\n",
    "print(f\"\\nTraining: {CONFIG['steps_per_iteration']} steps\")\n",
    "print(f\"Checkpoint: {checkpoint_dir}\")\n",
    "print()\n",
    "\n",
    "proposer_cmd = [\n",
    "    sys.executable, \"-m\", \"verl.trainer.main_ppo\",\n",
    "    \"--config-path\", str(REPO_DIR / \"config\"),\n",
    "    \"--config-name\", \"search_multiturn_grpo\",\n",
    "    \n",
    "    # Data\n",
    "    f\"data.train_files={CONFIG['train_parquet']}\",\n",
    "    f\"data.train_batch_size={CONFIG['micro_batch_size']}\",\n",
    "    f\"trainer.gradient_accumulation_steps={CONFIG['gradient_accumulation']}\",\n",
    "    \"data.max_prompt_length=1536\",\n",
    "    \"data.max_response_length=2560\",\n",
    "    f\"data.max_seq_length={CONFIG['proposer_max_seq_len']}\",\n",
    "    \n",
    "    # Model\n",
    "    f\"actor_rollout_ref.model.path={CONFIG['model_name']}\",\n",
    "    f\"actor_rollout_ref.actor.optim.lr={CONFIG['proposer_lr']}\",\n",
    "    \"actor_rollout_ref.actor.grad_clip=0.1\",\n",
    "    \"actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.03\",\n",
    "    \"actor_rollout_ref.actor.optim.weight_decay=0.01\",\n",
    "    \n",
    "    # HRPO: No ratio clipping, KL=0\n",
    "    \"algorithm.use_kl_in_reward=False\",\n",
    "    \"algorithm.adv_estimator=grpo_batch\",\n",
    "    f\"actor_rollout_ref.rollout.n={CONFIG['proposer_group_size']}\",\n",
    "    \"actor_rollout_ref.actor.use_kl_loss=False\",\n",
    "    # Note: No clip_ratio for HRPO (paper: \"omit ratio clipping\")\n",
    "    \n",
    "    # Single GPU\n",
    "    \"trainer.n_gpus_per_node=1\",\n",
    "    \"trainer.nnodes=1\",\n",
    "    \"actor_rollout_ref.rollout.tensor_model_parallel_size=1\",\n",
    "    \"actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2\",\n",
    "    \"actor_rollout_ref.rollout.gpu_memory_utilization=0.35\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    \"actor_rollout_ref.actor.fsdp_config.param_offload=True\",\n",
    "    \"actor_rollout_ref.actor.fsdp_config.optimizer_offload=True\",\n",
    "    \"actor_rollout_ref.model.enable_gradient_checkpointing=True\",\n",
    "    \n",
    "    # Multi-turn tool use\n",
    "    f\"actor_rollout_ref.rollout.multi_turn.tool_config_path={REPO_DIR}/config/search_tool_config.yaml\",\n",
    "    f\"actor_rollout_ref.rollout.max_turns={CONFIG['max_turns']}\",\n",
    "    \n",
    "    # Difficulty reward function\n",
    "    \"reward_model.reward_manager=batch\",\n",
    "    \"custom_reward_function.name=compute_biomedical_challenger_score_batch\",\n",
    "    \"custom_reward_function.path=verl/custom_reward/reward_function.py\",\n",
    "    f\"custom_reward_function.reward_kwargs.model_name={CONFIG['model_name']}\",\n",
    "    f\"custom_reward_function.reward_kwargs.base_url=http://127.0.0.1:{CONFIG['solver_port']}\",\n",
    "    f\"custom_reward_function.reward_kwargs.reward_rollout_n={CONFIG['reward_rollout_n']}\",\n",
    "    \n",
    "    # Training schedule\n",
    "    \"trainer.total_epochs=1\",\n",
    "    f\"trainer.total_training_steps={CONFIG['steps_per_iteration']}\",\n",
    "    f\"trainer.save_freq={CONFIG['save_freq']}\",\n",
    "    f\"trainer.default_hdfs_dir={checkpoint_dir}\",\n",
    "    f\"trainer.project_name=drzero-iter{ITERATION}\",\n",
    "    f\"trainer.experiment_name=proposer_iter{ITERATION}\",\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Expected time: 3-5 hours\\n\")\n",
    "\n",
    "try:\n",
    "    process = subprocess.run(proposer_cmd, cwd=str(REPO_DIR))\n",
    "    if process.returncode == 0:\n",
    "        print(f\"\\n[OK] Proposer Iteration {ITERATION} complete!\")\n",
    "        CONFIG[f'proposer_iter{ITERATION}_ckpt'] = str(checkpoint_dir / f'global_step_{CONFIG[\"steps_per_iteration\"]}')\n",
    "    else:\n",
    "        print(f\"\\n[FAIL] Training failed (exit code: {process.returncode})\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Generate Data with Iteration 1 Proposer\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ITERATION = 1\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "\n",
    "proposer_ckpt = CONFIG.get(f'proposer_iter{ITERATION}_ckpt')\n",
    "if not proposer_ckpt:\n",
    "    # Try to find checkpoint\n",
    "    ckpt_dir = Path(CONFIG['checkpoints_dir']) / f'iter{ITERATION}' / 'proposer'\n",
    "    ckpts = sorted(ckpt_dir.glob('global_step_*'))\n",
    "    if ckpts:\n",
    "        proposer_ckpt = str(ckpts[-1])\n",
    "        CONFIG[f'proposer_iter{ITERATION}_ckpt'] = proposer_ckpt\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No proposer checkpoint found for iteration {ITERATION}\")\n",
    "\n",
    "output_parquet = Path(CONFIG['checkpoints_dir']).parent / 'data' / f'iter{ITERATION}_generated.parquet'\n",
    "output_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ITERATION {ITERATION}: GENERATE DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProposer: {proposer_ckpt}\")\n",
    "print(f\"Output: {output_parquet}\")\n",
    "print(f\"Samples per seed: {CONFIG['solver_group_size']}\")\n",
    "print()\n",
    "\n",
    "gen_cmd = [\n",
    "    sys.executable, \"-m\", \"verl.trainer.main_generation\",\n",
    "    \"--config-path\", str(REPO_DIR / \"config\"),\n",
    "    \"--config-name\", \"search_multiturn_grpo\",\n",
    "    \n",
    "    f\"+ckpt_path={proposer_ckpt}\",\n",
    "    f\"+data.path={CONFIG['train_parquet']}\",\n",
    "    f\"+data.output_path={output_parquet}\",\n",
    "    \"+data.batch_size=64\",\n",
    "    \n",
    "    f\"actor_rollout_ref.rollout.n={CONFIG['solver_group_size']}\",\n",
    "    \"actor_rollout_ref.rollout.temperature=1.0\",\n",
    "    \"actor_rollout_ref.rollout.top_p=1.0\",\n",
    "    \"actor_rollout_ref.rollout.gpu_memory_utilization=0.8\",\n",
    "]\n",
    "\n",
    "print(\"Generating QA pairs...\")\n",
    "print(\"Expected time: 1-2 hours\\n\")\n",
    "\n",
    "try:\n",
    "    process = subprocess.run(gen_cmd, cwd=str(REPO_DIR))\n",
    "    if process.returncode == 0:\n",
    "        print(f\"\\n[OK] Data generation complete!\")\n",
    "        CONFIG[f'iter{ITERATION}_data'] = str(output_parquet)\n",
    "    else:\n",
    "        print(f\"\\n[FAIL] Generation failed\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nGeneration interrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Train Solver Iteration 1 (GRPO)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ITERATION = 1\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "\n",
    "generated_data = CONFIG.get(f'iter{ITERATION}_data')\n",
    "if not generated_data or not Path(generated_data).exists():\n",
    "    generated_data = str(Path(CONFIG['checkpoints_dir']).parent / 'data' / f'iter{ITERATION}_generated.parquet')\n",
    "\n",
    "checkpoint_dir = Path(CONFIG['checkpoints_dir']) / f'iter{ITERATION}' / 'solver'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ITERATION {ITERATION}: SOLVER TRAINING (GRPO)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAlgorithm: GRPO\")\n",
    "print(f\"  - WITH ratio clipping (epsilon={CONFIG['solver_clip_ratio']})\")\n",
    "print(f\"  - KL coefficient: {CONFIG['solver_kl_coef']}\")\n",
    "print(f\"  - Group size: {CONFIG['solver_group_size']}\")\n",
    "print(f\"\\nReward: Outcome-based (correctness)\")\n",
    "print(f\"\\nTraining data: {generated_data}\")\n",
    "print(f\"Checkpoint: {checkpoint_dir}\")\n",
    "print()\n",
    "\n",
    "solver_cmd = [\n",
    "    sys.executable, \"-m\", \"verl.trainer.main_ppo\",\n",
    "    \"--config-path\", str(REPO_DIR / \"config\"),\n",
    "    \"--config-name\", \"search_multiturn_grpo\",\n",
    "    \n",
    "    # Data\n",
    "    f\"data.train_files={generated_data}\",\n",
    "    f\"data.train_batch_size={CONFIG['micro_batch_size']}\",\n",
    "    f\"trainer.gradient_accumulation_steps={CONFIG['gradient_accumulation']}\",\n",
    "    \"data.max_prompt_length=512\",\n",
    "    f\"data.max_seq_length={CONFIG['solver_max_seq_len']}\",\n",
    "    \n",
    "    # Model\n",
    "    f\"actor_rollout_ref.model.path={CONFIG['model_name']}\",\n",
    "    f\"actor_rollout_ref.actor.optim.lr={CONFIG['solver_lr']}\",\n",
    "    \"actor_rollout_ref.actor.grad_clip=0.1\",\n",
    "    \"actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.03\",\n",
    "    \"actor_rollout_ref.actor.optim.weight_decay=0.01\",\n",
    "    \n",
    "    # GRPO: With ratio clipping\n",
    "    \"algorithm.adv_estimator=grpo\",\n",
    "    f\"actor_rollout_ref.rollout.n={CONFIG['solver_group_size']}\",\n",
    "    \"actor_rollout_ref.actor.use_kl_loss=True\",\n",
    "    f\"actor_rollout_ref.actor.kl_loss_coef={CONFIG['solver_kl_coef']}\",\n",
    "    f\"actor_rollout_ref.actor.clip_ratio={CONFIG['solver_clip_ratio']}\",\n",
    "    \n",
    "    # Single GPU\n",
    "    \"trainer.n_gpus_per_node=1\",\n",
    "    \"trainer.nnodes=1\",\n",
    "    \"actor_rollout_ref.rollout.tensor_model_parallel_size=1\",\n",
    "    \"actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2\",\n",
    "    \"actor_rollout_ref.rollout.gpu_memory_utilization=0.35\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    \"actor_rollout_ref.actor.fsdp_config.param_offload=True\",\n",
    "    \"actor_rollout_ref.actor.fsdp_config.optimizer_offload=True\",\n",
    "    \"actor_rollout_ref.model.enable_gradient_checkpointing=True\",\n",
    "    \n",
    "    # Multi-turn\n",
    "    f\"actor_rollout_ref.rollout.multi_turn.tool_config_path={REPO_DIR}/config/search_tool_config.yaml\",\n",
    "    f\"actor_rollout_ref.rollout.max_turns={CONFIG['max_turns']}\",\n",
    "    \n",
    "    # Training schedule\n",
    "    \"trainer.total_epochs=1\",\n",
    "    f\"trainer.total_training_steps={CONFIG['steps_per_iteration']}\",\n",
    "    f\"trainer.save_freq={CONFIG['save_freq']}\",\n",
    "    f\"trainer.default_hdfs_dir={checkpoint_dir}\",\n",
    "    f\"trainer.project_name=drzero-iter{ITERATION}\",\n",
    "    f\"trainer.experiment_name=solver_iter{ITERATION}\",\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Expected time: 3-4 hours\\n\")\n",
    "\n",
    "try:\n",
    "    process = subprocess.run(solver_cmd, cwd=str(REPO_DIR))\n",
    "    if process.returncode == 0:\n",
    "        print(f\"\\n[OK] Solver Iteration {ITERATION} complete!\")\n",
    "        CONFIG[f'solver_iter{ITERATION}_ckpt'] = str(checkpoint_dir / f'global_step_{CONFIG[\"steps_per_iteration\"]}')\n",
    "    else:\n",
    "        print(f\"\\n[FAIL] Training failed\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Convert Solver Iteration 1 to HuggingFace Format\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ITERATION = 1\n",
    "\n",
    "solver_ckpt = CONFIG.get(f'solver_iter{ITERATION}_ckpt')\n",
    "if not solver_ckpt:\n",
    "    ckpt_dir = Path(CONFIG['checkpoints_dir']) / f'iter{ITERATION}' / 'solver'\n",
    "    ckpts = sorted(ckpt_dir.glob('global_step_*'))\n",
    "    if ckpts:\n",
    "        solver_ckpt = str(ckpts[-1])\n",
    "\n",
    "output_hf_dir = Path(CONFIG['checkpoints_dir']) / f'iter{ITERATION}' / 'solver' / f'solver_iter{ITERATION}_hf'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"CONVERT SOLVER ITERATION {ITERATION} TO HF FORMAT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nInput: {solver_ckpt}\")\n",
    "print(f\"Output: {output_hf_dir}\")\n",
    "print()\n",
    "\n",
    "convert_cmd = [\n",
    "    sys.executable, \"-m\", \"verl.model_merger\", \"merge\",\n",
    "    \"--backend\", \"fsdp\",\n",
    "    \"--local_dir\", f\"{solver_ckpt}/actor\",\n",
    "    \"--target_dir\", str(output_hf_dir),\n",
    "]\n",
    "\n",
    "try:\n",
    "    process = subprocess.run(convert_cmd)\n",
    "    if process.returncode == 0:\n",
    "        print(f\"\\n[OK] Conversion complete!\")\n",
    "        CONFIG[f'solver_iter{ITERATION}_hf'] = str(output_hf_dir)\n",
    "        \n",
    "        # Verify\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        model = AutoModelForCausalLM.from_pretrained(output_hf_dir)\n",
    "        print(f\"  Model loaded successfully: {model.config.model_type}\")\n",
    "        del model\n",
    "    else:\n",
    "        print(f\"\\n[FAIL] Conversion failed\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
