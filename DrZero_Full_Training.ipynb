{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dr. Zero Full 3-Iteration Training (Google Colab)\n",
        "\n",
        "**Complete Implementation** of the Dr. Zero paper (arXiv:2601.07055)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements the **full 3-iteration self-evolution pipeline**:\n",
        "- **HRPO** (Hop-grouped Relative Policy Optimization) for proposer training\n",
        "- **GRPO** for solver training\n",
        "- **Difficulty-guided reward** (Paper Equation 4)\n",
        "- **4:3:2:1 hop ratio** for question distribution\n",
        "\n",
        "## Structure\n",
        "\n",
        "- **Cell 1**: Install all dependencies (run once, then restart runtime)\n",
        "- **Cell 2**: Full training pipeline (runs continuously until completion)\n",
        "- **Cell 3**: Evaluation and results\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Google Colab Pro+ (A100 80GB GPU)\n",
        "- 2TB Google Drive storage\n",
        "- ~28-42 hours total runtime\n",
        "\n",
        "## Paper Fidelity\n",
        "\n",
        "| Parameter | Paper | This Implementation |\n",
        "|-----------|-------|--------------------|\n",
        "| Algorithm (Proposer) | HRPO | HRPO |\n",
        "| Algorithm (Solver) | GRPO | GRPO |\n",
        "| Steps/Iteration | 50 | 50 |\n",
        "| Iterations | 3 | 3 |\n",
        "| Hop Ratio | 4:3:2:1 | 4:3:2:1 |\n",
        "| Reward Rollout N | 5 | 5 |\n",
        "| Effective Batch | 256 | 256 (32\u00d78) |\n",
        "\n",
        "**Only hardware parameters adapted** (8 GPU \u2192 1 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: INSTALL DEPENDENCIES\n",
        "# =============================================================================\n",
        "# Run this cell ONCE, then restart runtime (Runtime -> Restart session)\n",
        "# After restart, skip this cell and run Cell 2\n",
        "# =============================================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" INSTALLING DEPENDENCIES\")\n",
        "print(\" After completion, restart runtime and run Cell 2\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Mount Google Drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directory structure\n",
        "from pathlib import Path\n",
        "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_full')\n",
        "for subdir in ['corpus', 'data', 'checkpoints', 'logs']:\n",
        "    (DRIVE_BASE / subdir).mkdir(parents=True, exist_ok=True)\n",
        "for i in range(1, 4):\n",
        "    (DRIVE_BASE / 'checkpoints' / f'iter{i}' / 'proposer').mkdir(parents=True, exist_ok=True)\n",
        "    (DRIVE_BASE / 'checkpoints' / f'iter{i}' / 'solver').mkdir(parents=True, exist_ok=True)\n",
        "print(f\"\\n[OK] Directory structure created at {DRIVE_BASE}\")\n",
        "\n",
        "# Upgrade pip\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"])\n",
        "\n",
        "# Install numpy (version constraint for compatibility)\n",
        "print(\"\\n[1/7] Installing numpy...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy<2.0\"])\n",
        "\n",
        "# Install ML packages\n",
        "print(\"[2/7] Installing ML packages (torch, transformers, etc.)...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "    \"torch\", \"transformers\", \"accelerate\", \"datasets\", \"sentence-transformers\"])\n",
        "\n",
        "# Install utilities\n",
        "print(\"[3/7] Installing utilities...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "    \"biopython\", \"wandb\", \"tqdm\", \"psutil\", \"uvicorn\",\n",
        "    \"fastapi\", \"pydantic\", \"pandas\", \"pyarrow\", \"httpx\", \"openai\", \"requests\"])\n",
        "\n",
        "# Install FAISS (try GPU first, fall back to CPU)\n",
        "print(\"[4/7] Installing FAISS...\")\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-gpu\"],\n",
        "                         stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
        "    print(\"       Installed faiss-gpu\")\n",
        "except:\n",
        "    try:\n",
        "        import faiss\n",
        "        print(\"       faiss already available\")\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-cpu\"])\n",
        "        print(\"       Installed faiss-cpu\")\n",
        "\n",
        "# Install SGLang\n",
        "print(\"[5/7] Installing SGLang...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sglang[all]\"])\n",
        "\n",
        "# Install veRL\n",
        "print(\"[6/7] Installing veRL...\")\n",
        "if not os.path.exists('/content/verl'):\n",
        "    subprocess.check_call([\"git\", \"clone\", \"-q\",\n",
        "        \"https://github.com/volcengine/verl.git\", \"/content/verl\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/verl\"])\n",
        "\n",
        "# Install DrPubMedZero\n",
        "print(\"[7/7] Installing DrPubMedZero...\")\n",
        "if not os.path.exists('/content/DrPubMedZero'):\n",
        "    subprocess.check_call([\"git\", \"clone\", \"-q\",\n",
        "        \"https://github.com/ShivaAyyar/DrPubMedZero.git\", \"/content/DrPubMedZero\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/DrPubMedZero\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" INSTALLATION COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"  1. Runtime -> Restart session\")\n",
        "print(\"  2. Skip this cell\")\n",
        "print(\"  3. Run Cell 2 (Training)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: FULL TRAINING PIPELINE\n",
        "# =============================================================================\n",
        "# This cell runs the COMPLETE training pipeline:\n",
        "#   1. Mount Drive & verify GPU\n",
        "#   2. Download PubMed corpus (if needed)\n",
        "#   3. Build FAISS index (if needed)\n",
        "#   4. Prepare training seeds\n",
        "#   5. Run 3 iterations of Proposer-Solver training\n",
        "#   6. Save all checkpoints to Google Drive\n",
        "#\n",
        "# Total time: ~28-42 hours (runs continuously)\n",
        "# =============================================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import getpass\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify GPU\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"ERROR: No GPU available!\"\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f\"GPU: {gpu_name} ({gpu_mem:.0f}GB)\")\n",
        "assert gpu_mem >= 70, f\"ERROR: Need A100 80GB, got {gpu_mem:.0f}GB\"\n",
        "print(\"[OK] GPU verified\\n\")\n",
        "\n",
        "# Get user inputs\n",
        "print(\"=\"*70)\n",
        "print(\" CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "EMAIL = input(\"Enter email for NCBI API: \")\n",
        "assert '@' in EMAIL, \"Please enter a valid email\"\n",
        "\n",
        "# W&B setup (optional)\n",
        "wandb_key = getpass.getpass(\"W&B API key (press Enter to skip): \")\n",
        "if wandb_key.strip():\n",
        "    os.environ['WANDB_API_KEY'] = wandb_key\n",
        "    print(\"[OK] W&B configured\")\n",
        "else:\n",
        "    os.environ['WANDB_MODE'] = 'disabled'\n",
        "    print(\"[OK] W&B disabled\")\n",
        "\n",
        "# Configuration options\n",
        "print(\"\\nTraining Configuration (press Enter for defaults):\")\n",
        "\n",
        "corpus_input = input(\"Corpus size [200000]: \").strip()\n",
        "CORPUS_SIZE = int(corpus_input) if corpus_input else 200000\n",
        "\n",
        "seeds_input = input(\"Training seeds [5000]: \").strip()\n",
        "TRAINING_SEEDS = int(seeds_input) if seeds_input else 5000\n",
        "\n",
        "iter_input = input(\"Number of iterations [3]: \").strip()\n",
        "NUM_ITERATIONS = int(iter_input) if iter_input else 3\n",
        "\n",
        "steps_input = input(\"Steps per iteration [50]: \").strip()\n",
        "STEPS_PER_ITER = int(steps_input) if steps_input else 50\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Corpus size: {CORPUS_SIZE:,}\")\n",
        "print(f\"  Training seeds: {TRAINING_SEEDS:,}\")\n",
        "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
        "print(f\"  Steps/iteration: {STEPS_PER_ITER}\")\n",
        "print(f\"  Effective batch: 256 (32 x 8)\")\n",
        "\n",
        "# Update repo\n",
        "print(\"\\nUpdating repository...\")\n",
        "REPO_DIR = Path('/content/DrPubMedZero')\n",
        "if REPO_DIR.exists():\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], capture_output=True)\n",
        "else:\n",
        "    subprocess.check_call([\"git\", \"clone\",\n",
        "        \"https://github.com/ShivaAyyar/DrPubMedZero.git\", str(REPO_DIR)])\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.insert(0, str(REPO_DIR))\n",
        "print(f\"[OK] Repository ready at {REPO_DIR}\")\n",
        "\n",
        "# Verify custom modules\n",
        "print(\"\\nVerifying custom modules...\")\n",
        "try:\n",
        "    from verl.custom_reward.dr_zero_reward import compute_difficulty_reward\n",
        "    from verl.custom_reward.hrpo_advantage import compute_hrpo_advantages\n",
        "    from verl.custom_reward.hop_counter import count_hops\n",
        "    print(\"[OK] All custom modules loaded\")\n",
        "except ImportError as e:\n",
        "    print(f\"WARNING: {e}\")\n",
        "\n",
        "# Run training script\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" STARTING FULL TRAINING PIPELINE\")\n",
        "print(\" This will run continuously for ~28-42 hours\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Build command\n",
        "train_cmd = [\n",
        "    sys.executable, str(REPO_DIR / \"train_drzero_full.py\"),\n",
        "    f\"--email={EMAIL}\",\n",
        "    f\"--corpus_size={CORPUS_SIZE}\",\n",
        "    f\"--training_seeds={TRAINING_SEEDS}\",\n",
        "    f\"--iterations={NUM_ITERATIONS}\",\n",
        "    f\"--steps={STEPS_PER_ITER}\",\n",
        "    \"--drive_path=/content/drive/MyDrive/drzero_full\",\n",
        "    f\"--repo_dir={REPO_DIR}\"\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(train_cmd)}\\n\")\n",
        "\n",
        "# Run training\n",
        "try:\n",
        "    process = subprocess.run(train_cmd, cwd=str(REPO_DIR))\n",
        "    \n",
        "    if process.returncode == 0:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\" TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"\\nCheckpoints saved to:\")\n",
        "        for i in range(1, NUM_ITERATIONS + 1):\n",
        "            print(f\"  /content/drive/MyDrive/drzero_full/checkpoints/iter{i}/solver/solver_iter{i}_hf\")\n",
        "    else:\n",
        "        print(f\"\\nTraining exited with code: {process.returncode}\")\n",
        "        print(\"Check logs for errors.\")\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" TRAINING INTERRUPTED\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nCheckpoints have been saved to Google Drive.\")\n",
        "    print(\"To resume, re-run this cell.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: EVALUATION AND RESULTS\n",
        "# =============================================================================\n",
        "# Run this cell after training completes to:\n",
        "#   1. Verify all checkpoints\n",
        "#   2. Load and test the final model\n",
        "#   3. Generate sample outputs\n",
        "#   4. Display storage summary\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# Mount Drive if needed\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_full')\n",
        "CHECKPOINTS = DRIVE_BASE / 'checkpoints'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" TRAINING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. CHECKPOINT VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[1/4] Checkpoint Verification\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "found_checkpoints = []\n",
        "for i in range(1, 4):\n",
        "    hf_path = CHECKPOINTS / f'iter{i}' / 'solver' / f'solver_iter{i}_hf'\n",
        "    config_file = hf_path / 'config.json'\n",
        "    \n",
        "    if config_file.exists():\n",
        "        found_checkpoints.append((i, str(hf_path)))\n",
        "        # Get model size\n",
        "        total_size = sum(f.stat().st_size for f in hf_path.rglob('*') if f.is_file())\n",
        "        print(f\"  [OK] Iteration {i}: {hf_path.name} ({total_size/1e9:.1f} GB)\")\n",
        "    else:\n",
        "        print(f\"  [--] Iteration {i}: Not found\")\n",
        "\n",
        "if not found_checkpoints:\n",
        "    print(\"\\n  ERROR: No checkpoints found!\")\n",
        "    print(\"  Training may not have completed.\")\n",
        "else:\n",
        "    print(f\"\\n  Found {len(found_checkpoints)} checkpoint(s)\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. LOAD FINAL MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[2/4] Model Loading\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if found_checkpoints:\n",
        "    final_iter, final_path = found_checkpoints[-1]\n",
        "    print(f\"  Loading best model (Iteration {final_iter})...\")\n",
        "    \n",
        "    try:\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "        \n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            final_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        \n",
        "        print(f\"  [OK] Model loaded: {model.config.model_type}\")\n",
        "        print(f\"  Parameters: {model.num_parameters()/1e9:.2f}B\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] Failed to load model: {e}\")\n",
        "        model = None\n",
        "else:\n",
        "    model = None\n",
        "    print(\"  Skipping (no checkpoints)\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. GENERATE SAMPLE OUTPUT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[3/4] Sample Generation\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if model is not None:\n",
        "    test_prompts = [\n",
        "        \"Generate a challenging 2-hop biomedical question about BRCA1 mutations and breast cancer treatment.\",\n",
        "        \"What is the relationship between p53 tumor suppressor gene and cancer development?\",\n",
        "        \"Explain how insulin resistance leads to Type 2 diabetes.\"\n",
        "    ]\n",
        "    \n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n  Test {i}:\")\n",
        "        print(f\"  Prompt: {prompt[:60]}...\")\n",
        "        \n",
        "        try:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=200,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "            print(f\"  Response: {response[:150]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  [ERROR] Generation failed: {e}\")\n",
        "else:\n",
        "    print(\"  Skipping (no model loaded)\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. STORAGE SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[4/4] Storage Summary\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Calculate storage for each component\n",
        "components = [\n",
        "    (\"Corpus\", DRIVE_BASE / 'corpus'),\n",
        "    (\"Training Data\", DRIVE_BASE / 'data'),\n",
        "    (\"Checkpoints\", DRIVE_BASE / 'checkpoints'),\n",
        "    (\"Logs\", DRIVE_BASE / 'logs'),\n",
        "]\n",
        "\n",
        "total_used = 0\n",
        "for name, path in components:\n",
        "    if path.exists():\n",
        "        size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file())\n",
        "        total_used += size\n",
        "        print(f\"  {name}: {size/1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(f\"  {name}: --\")\n",
        "\n",
        "print(f\"  \" + \"-\"*30)\n",
        "print(f\"  Total: {total_used/1e9:.2f} GB\")\n",
        "\n",
        "# Drive space\n",
        "try:\n",
        "    total, used, free = shutil.disk_usage('/content/drive')\n",
        "    print(f\"\\n  Drive Space:\")\n",
        "    print(f\"    Total: {total/1e12:.1f} TB\")\n",
        "    print(f\"    Used: {used/1e12:.2f} TB\")\n",
        "    print(f\"    Free: {free/1e12:.2f} TB\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if found_checkpoints:\n",
        "    print(f\"\\nTraining Status: COMPLETE\")\n",
        "    print(f\"Best Model: Iteration {found_checkpoints[-1][0]}\")\n",
        "    print(f\"Model Path: {found_checkpoints[-1][1]}\")\n",
        "    print(f\"\\nTo use the model:\")\n",
        "    print(f\"  from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
        "    print(f\"  model = AutoModelForCausalLM.from_pretrained('{found_checkpoints[-1][1]}')\")\n",
        "    print(f\"  tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')\")\n",
        "else:\n",
        "    print(f\"\\nTraining Status: INCOMPLETE\")\n",
        "    print(\"No completed checkpoints found.\")\n",
        "    print(\"Re-run Cell 2 to continue training.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
