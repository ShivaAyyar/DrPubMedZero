{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Zero Biomedical Training (Lightweight Version)\n",
    "\n",
    "**Optimized for:** Single Google Drive (<15GB), A100 GPU (<20 hours)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains a lightweight version of Dr. Zero on PubMed biomedical literature:\n",
    "- **Corpus**: 10,000 papers (~2 GB)\n",
    "- **Training**: Single iteration only (~10-15 hours)\n",
    "- **Storage**: <15 GB on single Google Drive\n",
    "- **Checkpoints**: Temp storage + final checkpoint to Drive\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab Pro/Pro+ (A100 GPU)\n",
    "- <15 GB Google Drive storage\n",
    "- Weights & Biases account (optional)\n",
    "- Your email for NCBI PubMed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Mount Google Drive & Setup Directories\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Mount single Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory structure\n",
    "# Google Drive: Only final models and corpus (~12 GB)\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_lite')\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus'\n",
    "FINAL_CHECKPOINT_DIR = DRIVE_BASE / 'final_checkpoint'\n",
    "\n",
    "# Local temp: Intermediate checkpoints, working files (auto-deleted on disconnect)\n",
    "LOCAL_BASE = Path('/content/drzero_temp')\n",
    "LOCAL_CHECKPOINT = LOCAL_BASE / 'checkpoints'\n",
    "LOCAL_DATA = LOCAL_BASE / 'data'\n",
    "LOCAL_LOGS = LOCAL_BASE / 'logs'\n",
    "\n",
    "# Create all directories\n",
    "for dir_path in [DRIVE_BASE, CORPUS_DIR, FINAL_CHECKPOINT_DIR,\n",
    "                 LOCAL_BASE, LOCAL_CHECKPOINT, LOCAL_DATA, LOCAL_LOGS]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Storage Layout:\")\n",
    "print(f\"  Google Drive (~12 GB): {DRIVE_BASE}\")\n",
    "print(f\"    - Corpus: {CORPUS_DIR}\")\n",
    "print(f\"    - Final model: {FINAL_CHECKPOINT_DIR}\")\n",
    "print(f\"  Local temp (auto-deleted): {LOCAL_BASE}\")\n",
    "print(f\"    - Working checkpoints: {LOCAL_CHECKPOINT}\")\n",
    "print(f\"    - Data: {LOCAL_DATA}\")\n",
    "print(\"\\n‚úÖ Directories created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 2: Install Dependencies\n\nimport subprocess\nimport sys\nimport os\n\n# Standard packages\npackages = [\n    \"torch\", \"transformers\", \"accelerate\", \"datasets\",\n    \"sentence-transformers\", \"biopython\",\n    \"wandb\", \"tqdm\", \"psutil\"\n]\n\nprint(\"Installing dependencies...\")\nfor pkg in packages:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n    print(f\"  ‚úì {pkg}\")\n\n# Install FAISS - use faiss-cpu which is more reliable\n# (GPU acceleration still works via PyTorch for embeddings)\nprint(\"Installing FAISS...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-cpu\"])\nprint(f\"  ‚úì faiss-cpu\")\n\n# Install SGLang (may take a moment)\nprint(\"Installing SGLang...\")\ntry:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sglang[all]\"],\n                         stderr=subprocess.DEVNULL)\n    print(f\"  ‚úì sglang\")\nexcept:\n    # Fallback: install without extras\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sglang\"])\n    print(f\"  ‚úì sglang (minimal)\")\n\n# Install veRL\nif not os.path.exists('/content/verl'):\n    print(\"Installing veRL framework...\")\n    subprocess.check_call([\"git\", \"clone\", \"-q\", \"https://github.com/volcengine/verl.git\", \"/content/verl\"])\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/verl\"])\n    print(f\"  ‚úì verl\")\nelse:\n    print(f\"  ‚úì verl (already installed)\")\n\n# Verify GPU\nimport torch\nassert torch.cuda.is_available(), \"No GPU detected! Set Runtime -> A100 GPU\"\nprint(f\"\\n‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Clone Repository\n",
    "\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    subprocess.check_call([\n",
    "        \"git\", \"clone\",\n",
    "        \"https://github.com/ShivaAyyar/DrPubMedZero.git\",\n",
    "        str(REPO_DIR)\n",
    "    ])\n",
    "else:\n",
    "    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"pull\"])\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "# Verify biomedical module\n",
    "from biomedical import PubMedCorpusManager, setup_for_colab\n",
    "setup_for_colab()\n",
    "\n",
    "print(f\"‚úÖ Repository ready: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Configuration (Lightweight Version)\n",
    "\n",
    "import getpass\n",
    "\n",
    "CONFIG = {\n",
    "    # Reduced scale to fit in 15GB and <20 GPU hours\n",
    "    'model_name': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    \n",
    "    # Data - REDUCED\n",
    "    'corpus_size': 10000,  # 10K papers instead of 50K (~2 GB)\n",
    "    'training_seeds': 500,  # 500 seeds instead of 2000\n",
    "    'pubmed_query': '(breast cancer OR lung cancer) AND (gene OR protein)',\n",
    "    'date_range': ('2022/01/01', '2024/12/31'),  # Recent papers only\n",
    "    \n",
    "    # Training - REDUCED for <20 GPU hours\n",
    "    'batch_size': 32,  # Smaller batch\n",
    "    'gradient_accumulation': 4,\n",
    "    'learning_rate': 1e-6,\n",
    "    'max_steps': 100,  # ~10-15 hours on A100\n",
    "    'save_freq': 50,  # Save every 50 steps (only 2 checkpoints)\n",
    "    \n",
    "    # Paths\n",
    "    'corpus_path': str(CORPUS_DIR),\n",
    "    'final_checkpoint': str(FINAL_CHECKPOINT_DIR),\n",
    "    'temp_checkpoint': str(LOCAL_CHECKPOINT),\n",
    "    'data_dir': str(LOCAL_DATA),\n",
    "    'logs_dir': str(LOCAL_LOGS),\n",
    "    \n",
    "    # Servers\n",
    "    'retrieval_port': 8000,\n",
    "    'solver_port': 8001,\n",
    "}\n",
    "\n",
    "# User inputs\n",
    "CONFIG['email'] = 'ssa163@case.edu'\n",
    "print(f\"Email: {CONFIG['email']}\")\n",
    "\n",
    "wandb_key = getpass.getpass(\"W&B API key (optional, press Enter to skip): \")\n",
    "if wandb_key:\n",
    "    os.environ['WANDB_API_KEY'] = wandb_key\n",
    "    import wandb\n",
    "    wandb.login(key=wandb_key)\n",
    "else:\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "print(\"\\nüìã Configuration:\")\n",
    "print(f\"  Corpus: {CONFIG['corpus_size']} papers (~2 GB)\")\n",
    "print(f\"  Training: {CONFIG['max_steps']} steps (~10-15 hours)\")\n",
    "print(f\"  Storage: <15 GB total\")\n",
    "print(f\"\\n  Drive: {DRIVE_BASE}\")\n",
    "print(f\"  Temp: {LOCAL_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Download PubMed Corpus (Lightweight)\n",
    "\n",
    "from biomedical import PubMedCorpusManager\n",
    "import json\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_path']) / 'pubmed-corpus-lite.jsonl'\n",
    "\n",
    "if corpus_file.exists():\n",
    "    with open(corpus_file) as f:\n",
    "        n_papers = sum(1 for _ in f)\n",
    "    print(f\"‚úì Corpus exists: {n_papers} papers\")\n",
    "    if n_papers >= CONFIG['corpus_size']:\n",
    "        print(\"  Skipping download\")\n",
    "    else:\n",
    "        download = True\n",
    "else:\n",
    "    print(f\"Downloading {CONFIG['corpus_size']} PubMed papers...\")\n",
    "    print(\"‚è±Ô∏è Estimated time: 15-20 minutes\")\n",
    "    download = True\n",
    "\n",
    "if download:\n",
    "    manager = PubMedCorpusManager(\n",
    "        save_path=CONFIG['corpus_path'],\n",
    "        email=CONFIG['email']\n",
    "    )\n",
    "    \n",
    "    articles = manager.download_pubmed_abstracts(\n",
    "        query=CONFIG['pubmed_query'],\n",
    "        max_results=CONFIG['corpus_size'],\n",
    "        date_range=CONFIG['date_range']\n",
    "    )\n",
    "    \n",
    "    manager.save_corpus(articles)\n",
    "    print(f\"\\n‚úÖ Downloaded {len(articles)} papers!\")\n",
    "\n",
    "CONFIG['corpus_file'] = str(corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 6: Build FAISS Index\n\nfrom biomedical import build_biomedical_index\n\nindex_path = Path(CONFIG['corpus_path']) / 'index-lite.faiss'\n\nif index_path.exists():\n    print(\"‚úì Index exists, skipping build\")\nelse:\n    print(\"Building PubMedBERT index...\")\n    print(\"‚è±Ô∏è Estimated time: 10-15 minutes\")\n    \n    build_biomedical_index(\n        corpus_file=str(corpus_file),\n        index_save_path=str(index_path),\n        use_gpu=False,  # Using faiss-cpu\n        batch_size=64\n    )\n    \n    size_mb = os.path.getsize(index_path) / (1024**2)\n    print(f\"\\n‚úÖ Index built: {size_mb:.0f} MB\")\n\nCONFIG['index_path'] = str(index_path)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Prepare Training Seeds\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "seeds_file = Path(CONFIG['data_dir']) / 'seeds.jsonl'\n",
    "\n",
    "if seeds_file.exists():\n",
    "    print(\"‚úì Seeds exist, skipping\")\n",
    "else:\n",
    "    # Load corpus\n",
    "    corpus = []\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    \n",
    "    # Sample seeds (prefer longer abstracts)\n",
    "    substantial = [p for p in corpus if len(p.get('abstract', '').split()) > 100]\n",
    "    seeds = random.sample(\n",
    "        substantial if len(substantial) >= CONFIG['training_seeds'] else corpus,\n",
    "        CONFIG['training_seeds']\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    with open(seeds_file, 'w') as f:\n",
    "        for seed in seeds:\n",
    "            f.write(json.dumps(seed) + '\\n')\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(seeds)} training seeds\")\n",
    "\n",
    "CONFIG['seeds_file'] = str(seeds_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 8: Launch Servers\n\nfrom colab_helpers import BackgroundServer, kill_port\nimport time\n\n# Kill existing processes\nkill_port(CONFIG['retrieval_port'])\nkill_port(CONFIG['solver_port'])\ntime.sleep(2)\n\n# Launch retrieval server\nprint(\"Starting retrieval server...\")\nretrieval_server = BackgroundServer(\n    name=\"retrieval\",\n    log_file=str(Path(CONFIG['logs_dir']) / 'retrieval.log')\n)\n\nretrieval_server.start([\n    \"python\", \"search/retrieval_server.py\",\n    \"--mode\", \"biomedical\",\n    \"--corpus_path\", CONFIG['corpus_file'],\n    \"--index_path\", CONFIG['index_path'],\n    \"--port\", str(CONFIG['retrieval_port'])\n], cwd=str(REPO_DIR))\n\nassert retrieval_server.wait_until_ready(\n    f\"http://localhost:{CONFIG['retrieval_port']}/health\",\n    timeout=180\n), \"Retrieval server failed to start\"\n\nprint(\"‚úì Retrieval server ready\")\n\n# Launch solver server\nprint(\"\\nStarting solver server...\")\nsolver_server = BackgroundServer(\n    name=\"solver\",\n    log_file=str(Path(CONFIG['logs_dir']) / 'solver.log')\n)\n\nsolver_server.start([\n    \"python\", \"-m\", \"sglang.launch_server\",\n    \"--model-path\", CONFIG['model_name'],\n    \"--port\", str(CONFIG['solver_port']),\n    \"--tp\", \"1\",\n    \"--mem-fraction-static\", \"0.5\"\n])\n\nassert solver_server.wait_until_ready(\n    f\"http://localhost:{CONFIG['solver_port']}/health\",\n    timeout=300\n), \"Solver server failed to start\"\n\nprint(\"‚úì Solver server ready\")\nprint(\"\\n‚úÖ Both servers operational!\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 9: Train Proposer (Single Iteration)\n",
    "\n",
    "os.environ['RETRIEVAL_SERVER_URL'] = f\"http://localhost:{CONFIG['retrieval_port']}\"\n",
    "os.environ['SOLVER_SERVER_URL'] = f\"http://localhost:{CONFIG['solver_port']}\"\n",
    "os.environ['NCBI_EMAIL'] = CONFIG['email']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING PROPOSER (Lightweight - Single Iteration)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSteps: {CONFIG['max_steps']}\")\n",
    "print(f\"Estimated time: 10-15 hours on A100\")\n",
    "print(f\"\\nCheckpoints saved to temp: {CONFIG['temp_checkpoint']}\")\n",
    "print(f\"Final checkpoint copied to Drive: {CONFIG['final_checkpoint']}\\n\")\n",
    "\n",
    "# Training command\n",
    "train_cmd = [\n",
    "    \"python\", \"-m\", \"verl.trainer.main_ppo\",\n",
    "    \"--config-path\", \"./config\",\n",
    "    \"--config-name\", \"search_multiturn_grpo\",\n",
    "    f\"actor_rollout_ref.model.path={CONFIG['model_name']}\",\n",
    "    f\"actor_rollout_ref.actor.optim.lr={CONFIG['learning_rate']}\",\n",
    "    f\"data.train_files=[{CONFIG['seeds_file']}]\",\n",
    "    f\"data.train_batch_size={CONFIG['batch_size']}\",\n",
    "    \"trainer.n_gpus_per_node=1\",\n",
    "    f\"trainer.gradient_accumulation_steps={CONFIG['gradient_accumulation']}\",\n",
    "    f\"trainer.save_freq={CONFIG['save_freq']}\",\n",
    "    f\"trainer.total_training_steps={CONFIG['max_steps']}\",\n",
    "    f\"trainer.default_hdfs_dir={CONFIG['temp_checkpoint']}\",\n",
    "    \"trainer.project_name=drzero-lite\",\n",
    "    \"actor_rollout_ref.rollout.tensor_model_parallel_size=1\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    subprocess.run(train_cmd, cwd=str(REPO_DIR), check=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find final checkpoint\n",
    "    checkpoints = sorted(Path(CONFIG['temp_checkpoint']).glob(\"step_*\"))\n",
    "    if checkpoints:\n",
    "        final_ckpt = checkpoints[-1]\n",
    "        \n",
    "        # Copy to Drive for persistence\n",
    "        import shutil\n",
    "        print(f\"\\nCopying final checkpoint to Drive...\")\n",
    "        shutil.copytree(final_ckpt, CONFIG['final_checkpoint'] / final_ckpt.name, dirs_exist_ok=True)\n",
    "        \n",
    "        print(f\"‚úÖ Final model saved to: {CONFIG['final_checkpoint'] / final_ckpt.name}\")\n",
    "        print(f\"\\nüí° Intermediate checkpoints in temp storage will be deleted on disconnect\")\n",
    "        print(f\"   Only final checkpoint is saved to Google Drive\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    print(\"\\nCheck logs:\")\n",
    "    print(f\"  - W&B dashboard\")\n",
    "    print(f\"  - {CONFIG['logs_dir']}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 10: Check Storage Usage\n",
    "\n",
    "import shutil\n",
    "\n",
    "def get_size(path):\n",
    "    total = 0\n",
    "    for entry in Path(path).rglob('*'):\n",
    "        if entry.is_file():\n",
    "            total += entry.stat().st_size\n",
    "    return total / (1024**3)  # Convert to GB\n",
    "\n",
    "print(\"Storage Usage:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "drive_size = get_size(DRIVE_BASE)\n",
    "print(f\"Google Drive: {drive_size:.2f} GB\")\n",
    "print(f\"  - Corpus: ~2 GB\")\n",
    "print(f\"  - Index: ~0.5 GB\")\n",
    "print(f\"  - Final checkpoint: ~{drive_size - 2.5:.1f} GB\")\n",
    "\n",
    "temp_size = get_size(LOCAL_BASE)\n",
    "print(f\"\\nTemp storage: {temp_size:.2f} GB (will be deleted)\")\n",
    "\n",
    "print(f\"\\nTotal Drive usage: {drive_size:.1f} GB / 15 GB\")\n",
    "if drive_size < 15:\n",
    "    print(\"‚úÖ Within 15 GB limit!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Exceeds 15 GB - consider deleting old files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Complete!\n",
    "\n",
    "## What You Have\n",
    "\n",
    "- ‚úÖ PubMed corpus (10K papers, ~2 GB)\n",
    "- ‚úÖ PubMedBERT search index (~0.5 GB)\n",
    "- ‚úÖ Trained proposer model (~8-10 GB)\n",
    "- ‚úÖ **Total**: <15 GB on Google Drive\n",
    "\n",
    "## GPU Time Used\n",
    "\n",
    "- Corpus download: ~20 min\n",
    "- Index building: ~15 min\n",
    "- Training: ~10-15 hours\n",
    "- **Total**: <20 hours on A100\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Download model**: `!zip -r model.zip {CONFIG['final_checkpoint']}`\n",
    "2. **Test generation**: Use proposer to generate biomedical questions\n",
    "3. **Extend training**: Add more steps or iterations if needed\n",
    "\n",
    "## Storage Cleanup\n",
    "\n",
    "To free up space:\n",
    "- Temp files auto-delete on disconnect\n",
    "- Keep only final checkpoint on Drive\n",
    "- Archive corpus if needed: `!tar -czf corpus.tar.gz {CORPUS_DIR}`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}