{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Zero Biomedical Training (Lightweight Version)\n",
    "\n",
    "**Optimized for:** Single Google Drive (<15GB), A100 GPU (<20 hours)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains a lightweight version of Dr. Zero on PubMed biomedical literature:\n",
    "- **Corpus**: 10,000 papers (~2 GB)\n",
    "- **Training**: Single iteration only (~10-15 hours)\n",
    "- **Storage**: <15 GB on single Google Drive\n",
    "- **Checkpoints**: Temp storage + final checkpoint to Drive\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab Pro/Pro+ (A100 GPU)\n",
    "- <15 GB Google Drive storage\n",
    "- Weights & Biases account (optional)\n",
    "- Your email for NCBI PubMed API\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Run Cell 1 (Mount Drive)\n",
    "2. Run Cell 2 (Install Dependencies) - **THEN RESTART RUNTIME**\n",
    "3. Run Cell 1 again (re-mount Drive after restart)\n",
    "4. Skip Cell 2, continue from Cell 3 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Mount Google Drive & Setup Directories\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Mount single Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory structure\n",
    "# Google Drive: Only final models and corpus (~12 GB)\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_lite')\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus'\n",
    "FINAL_CHECKPOINT_DIR = DRIVE_BASE / 'final_checkpoint'\n",
    "\n",
    "# Local temp: Intermediate checkpoints, working files (auto-deleted on disconnect)\n",
    "LOCAL_BASE = Path('/content/drzero_temp')\n",
    "LOCAL_CHECKPOINT = LOCAL_BASE / 'checkpoints'\n",
    "LOCAL_DATA = LOCAL_BASE / 'data'\n",
    "LOCAL_LOGS = LOCAL_BASE / 'logs'\n",
    "\n",
    "# Create all directories\n",
    "for dir_path in [DRIVE_BASE, CORPUS_DIR, FINAL_CHECKPOINT_DIR,\n",
    "                 LOCAL_BASE, LOCAL_CHECKPOINT, LOCAL_DATA, LOCAL_LOGS]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Storage Layout:\")\n",
    "print(f\"  Google Drive (~12 GB): {DRIVE_BASE}\")\n",
    "print(f\"    - Corpus: {CORPUS_DIR}\")\n",
    "print(f\"    - Final model: {FINAL_CHECKPOINT_DIR}\")\n",
    "print(f\"  Local temp (auto-deleted): {LOCAL_BASE}\")\n",
    "print(f\"    - Working checkpoints: {LOCAL_CHECKPOINT}\")\n",
    "print(f\"    - Data: {LOCAL_DATA}\")\n",
    "print(\"\\n[OK] Directories created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 2: Install Dependencies\n",
    "# IMPORTANT: After this cell completes, go to Runtime -> Restart session\n",
    "# Then skip this cell and continue from Cell 3\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "print(\"This may take 5-10 minutes.\\n\")\n",
    "\n",
    "# Upgrade pip first\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"])\n",
    "\n",
    "# Install numpy first to avoid binary incompatibility\n",
    "print(\"1/5 Installing numpy...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy<2.0\"])\n",
    "\n",
    "# Core ML packages\n",
    "print(\"2/5 Installing ML packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
    "    \"torch\", \"transformers\", \"accelerate\", \"datasets\", \"sentence-transformers\"])\n",
    "\n",
    "# Utility packages\n",
    "print(\"3/5 Installing utilities...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"biopython\", \"wandb\", \"tqdm\", \"psutil\", \"faiss-cpu\", \"uvicorn\", \"fastapi\", \"pydantic\"])\n",
    "\n",
    "# Install vLLM for model serving (alternative to SGLang which has issues)\n",
    "print(\"4/5 Installing vLLM...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"vllm\"])\n",
    "\n",
    "# Clone and install veRL\n",
    "print(\"5/5 Installing veRL...\")\n",
    "if not os.path.exists('/content/verl'):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"-q\", \"https://github.com/volcengine/verl.git\", \"/content/verl\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/verl\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTANT: RESTART RUNTIME NOW\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Go to: Runtime -> Restart session\")\n",
    "print(\"2. After restart, run Cell 1 again (mount drive)\")\n",
    "print(\"3. Then SKIP this cell and continue from Cell 3\")\n",
    "print(\"\\nThis restart is required for numpy to load correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Clone Repository & Verify Setup\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify numpy loaded correctly\n",
    "print(\"Verifying installations...\")\n",
    "import numpy as np\n",
    "print(f\"  [OK] numpy {np.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"  [OK] torch {torch.__version__}\")\n",
    "assert torch.cuda.is_available(), \"ERROR: No GPU! Go to Runtime -> Change runtime type -> A100 GPU\"\n",
    "print(f\"  [OK] GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"  [OK] transformers {transformers.__version__}\")\n",
    "\n",
    "import faiss\n",
    "print(f\"  [OK] faiss\")\n",
    "\n",
    "# Clone repository\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "\n",
    "print(\"\\nSetting up repository...\")\n",
    "if not REPO_DIR.exists():\n",
    "    subprocess.check_call([\n",
    "        \"git\", \"clone\",\n",
    "        \"https://github.com/ShivaAyyar/DrPubMedZero.git\",\n",
    "        str(REPO_DIR)\n",
    "    ])\n",
    "    print(\"  [OK] Repository cloned\")\n",
    "else:\n",
    "    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"pull\"], \n",
    "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print(\"  [OK] Repository updated\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "# Verify biomedical module\n",
    "from biomedical import PubMedCorpusManager\n",
    "print(\"  [OK] Biomedical module loaded\")\n",
    "\n",
    "print(f\"\\n[OK] All checks passed! Repository: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Configuration\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "# Re-establish directory paths (in case of restart)\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_lite')\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus'\n",
    "FINAL_CHECKPOINT_DIR = DRIVE_BASE / 'final_checkpoint'\n",
    "LOCAL_BASE = Path('/content/drzero_temp')\n",
    "LOCAL_CHECKPOINT = LOCAL_BASE / 'checkpoints'\n",
    "LOCAL_DATA = LOCAL_BASE / 'data'\n",
    "LOCAL_LOGS = LOCAL_BASE / 'logs'\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [CORPUS_DIR, FINAL_CHECKPOINT_DIR, LOCAL_CHECKPOINT, LOCAL_DATA, LOCAL_LOGS]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    \n",
    "    # Data - Lightweight settings\n",
    "    'corpus_size': 10000,\n",
    "    'training_seeds': 500,\n",
    "    'pubmed_query': '(breast cancer OR lung cancer) AND (gene OR protein)',\n",
    "    'date_range': ('2022/01/01', '2024/12/31'),\n",
    "    \n",
    "    # Training - Lightweight settings\n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation': 4,\n",
    "    'learning_rate': 1e-6,\n",
    "    'max_steps': 100,\n",
    "    'save_freq': 50,\n",
    "    \n",
    "    # Paths\n",
    "    'corpus_dir': str(CORPUS_DIR),\n",
    "    'corpus_file': str(CORPUS_DIR / 'pubmed-corpus.jsonl'),\n",
    "    'index_file': str(CORPUS_DIR / 'pubmedbert_index.faiss'),\n",
    "    'final_checkpoint': str(FINAL_CHECKPOINT_DIR),\n",
    "    'temp_checkpoint': str(LOCAL_CHECKPOINT),\n",
    "    'data_dir': str(LOCAL_DATA),\n",
    "    'logs_dir': str(LOCAL_LOGS),\n",
    "    \n",
    "    # Server ports\n",
    "    'retrieval_port': 8000,\n",
    "    'solver_port': 8001,\n",
    "}\n",
    "\n",
    "# User email for NCBI\n",
    "CONFIG['email'] = 'ssa163@case.edu'\n",
    "print(f\"NCBI Email: {CONFIG['email']}\")\n",
    "\n",
    "# Weights & Biases (optional)\n",
    "wandb_key = getpass.getpass(\"W&B API key (press Enter to skip): \")\n",
    "if wandb_key.strip():\n",
    "    os.environ['WANDB_API_KEY'] = wandb_key\n",
    "    import wandb\n",
    "    wandb.login(key=wandb_key)\n",
    "    print(\"  [OK] W&B configured\")\n",
    "else:\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "    print(\"  [OK] W&B disabled\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Corpus: {CONFIG['corpus_size']} papers\")\n",
    "print(f\"  Training: {CONFIG['max_steps']} steps\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"\\n[OK] Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Download PubMed Corpus\n",
    "\n",
    "from biomedical import PubMedCorpusManager\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "\n",
    "# Check if corpus already exists\n",
    "if corpus_file.exists():\n",
    "    with open(corpus_file) as f:\n",
    "        n_papers = sum(1 for _ in f)\n",
    "    print(f\"Corpus already exists: {n_papers} papers\")\n",
    "    \n",
    "    if n_papers >= CONFIG['corpus_size']:\n",
    "        print(\"Skipping download.\")\n",
    "        download_needed = False\n",
    "    else:\n",
    "        print(f\"Need {CONFIG['corpus_size'] - n_papers} more papers\")\n",
    "        download_needed = True\n",
    "else:\n",
    "    print(f\"Downloading {CONFIG['corpus_size']} PubMed papers...\")\n",
    "    print(\"Estimated time: 15-30 minutes\\n\")\n",
    "    download_needed = True\n",
    "\n",
    "if download_needed:\n",
    "    manager = PubMedCorpusManager(\n",
    "        save_path=CONFIG['corpus_dir'],\n",
    "        email=CONFIG['email']\n",
    "    )\n",
    "    \n",
    "    articles = manager.download_pubmed_abstracts(\n",
    "        query=CONFIG['pubmed_query'],\n",
    "        max_results=CONFIG['corpus_size'],\n",
    "        date_range=CONFIG['date_range']\n",
    "    )\n",
    "    \n",
    "    if articles:\n",
    "        manager.save_corpus(articles)\n",
    "        print(f\"\\n[OK] Downloaded {len(articles)} papers!\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Download failed - check internet connection and email\")\n",
    "else:\n",
    "    print(\"[OK] Using existing corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Build FAISS Index\n",
    "\n",
    "from biomedical.biomedical_retriever import BiomedicalRetrieverServer\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "index_file = Path(CONFIG['index_file'])\n",
    "\n",
    "if index_file.exists():\n",
    "    print(f\"Index already exists: {index_file}\")\n",
    "    print(\"Skipping index build.\")\n",
    "else:\n",
    "    print(\"Building PubMedBERT FAISS index...\")\n",
    "    print(\"Estimated time: 10-20 minutes\\n\")\n",
    "    \n",
    "    # The BiomedicalRetrieverServer will build and save the index\n",
    "    retriever = BiomedicalRetrieverServer(\n",
    "        corpus_path=str(corpus_file),\n",
    "        index_path=str(index_file),\n",
    "        model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        device=\"cuda\",\n",
    "        topk=3\n",
    "    )\n",
    "    \n",
    "    # Test search\n",
    "    print(\"\\nTesting search...\")\n",
    "    results = retriever.search(\"TP53 mutation breast cancer\")\n",
    "    print(f\"  Found {len(results)} results\")\n",
    "    \n",
    "    # Clean up to free GPU memory\n",
    "    del retriever\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    size_mb = os.path.getsize(index_file) / (1024**2)\n",
    "    print(f\"\\n[OK] Index built: {size_mb:.0f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Prepare Training Seeds\n",
    "\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "seeds_file = Path(CONFIG['data_dir']) / 'seeds.jsonl'\n",
    "\n",
    "if seeds_file.exists():\n",
    "    with open(seeds_file) as f:\n",
    "        n_seeds = sum(1 for _ in f)\n",
    "    print(f\"Seeds already exist: {n_seeds}\")\n",
    "    if n_seeds >= CONFIG['training_seeds']:\n",
    "        print(\"Skipping seed preparation.\")\n",
    "        prepare_needed = False\n",
    "    else:\n",
    "        prepare_needed = True\n",
    "else:\n",
    "    prepare_needed = True\n",
    "\n",
    "if prepare_needed:\n",
    "    print(f\"Preparing {CONFIG['training_seeds']} training seeds...\")\n",
    "    \n",
    "    # Load corpus\n",
    "    corpus = []\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    print(f\"  Loaded {len(corpus)} papers from corpus\")\n",
    "    \n",
    "    # Filter for papers with substantial abstracts\n",
    "    substantial = [p for p in corpus if len(p.get('abstract', '').split()) > 100]\n",
    "    print(f\"  {len(substantial)} papers have >100 word abstracts\")\n",
    "    \n",
    "    # Sample seeds\n",
    "    source = substantial if len(substantial) >= CONFIG['training_seeds'] else corpus\n",
    "    seeds = random.sample(source, min(CONFIG['training_seeds'], len(source)))\n",
    "    \n",
    "    # Save seeds\n",
    "    with open(seeds_file, 'w') as f:\n",
    "        for seed in seeds:\n",
    "            f.write(json.dumps(seed) + '\\n')\n",
    "    \n",
    "    print(f\"\\n[OK] Saved {len(seeds)} training seeds\")\n",
    "else:\n",
    "    print(\"[OK] Using existing seeds\")\n",
    "\n",
    "CONFIG['seeds_file'] = str(seeds_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Test Retrieval & Model Loading\n",
    "# This cell tests that everything works before starting the long training\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRE-TRAINING VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Load and test retriever\n",
    "print(\"\\n1. Testing retriever...\")\n",
    "from biomedical.biomedical_retriever import BiomedicalRetrieverServer\n",
    "import torch\n",
    "\n",
    "retriever = BiomedicalRetrieverServer(\n",
    "    corpus_path=CONFIG['corpus_file'],\n",
    "    index_path=CONFIG['index_file'],\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "results = retriever.search(\"breast cancer chemotherapy resistance\")\n",
    "print(f\"   [OK] Retriever working - found {len(results)} results\")\n",
    "if results:\n",
    "    print(f\"   Sample: {results[0].get('title', 'N/A')[:60]}...\")\n",
    "\n",
    "# Clean up retriever\n",
    "del retriever\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Test 2: Load model\n",
    "print(\"\\n2. Testing model loading...\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"   Loading {CONFIG['model_name']}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Quick generation test\n",
    "inputs = tokenizer(\"What is breast cancer?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"   [OK] Model generating: {response[:50]}...\")\n",
    "\n",
    "# Clean up\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Test 3: Check GPU memory\n",
    "print(\"\\n3. GPU Memory Status:\")\n",
    "total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "used = torch.cuda.memory_allocated() / 1e9\n",
    "free = total - used\n",
    "print(f\"   Total: {total:.1f} GB\")\n",
    "print(f\"   Used: {used:.1f} GB\")\n",
    "print(f\"   Free: {free:.1f} GB\")\n",
    "\n",
    "# Test 4: Check files\n",
    "print(\"\\n4. Required Files:\")\n",
    "from pathlib import Path\n",
    "files_ok = True\n",
    "for name, path in [('Corpus', CONFIG['corpus_file']), \n",
    "                   ('Index', CONFIG['index_file']),\n",
    "                   ('Seeds', CONFIG['seeds_file'])]:\n",
    "    exists = Path(path).exists()\n",
    "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
    "    print(f\"   {status} {name}: {path}\")\n",
    "    if not exists:\n",
    "        files_ok = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if files_ok:\n",
    "    print(\"ALL CHECKS PASSED - Ready for training!\")\n",
    "    print(\"\\nNext: Run Cell 9 to start training.\")\n",
    "    print(\"Training will take approximately 10-15 hours on A100.\")\n",
    "else:\n",
    "    print(\"SOME CHECKS FAILED - Please fix issues above before training.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 9: Train Biomedical Proposer (Simplified for Colab)\n# Uses HuggingFace TRL instead of veRL for single-GPU compatibility\n\nimport os\nimport sys\nimport json\nimport torch\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nfrom datasets import Dataset\n\nprint(\"=\"*60)\nprint(\"BIOMEDICAL PROPOSER TRAINING\")\nprint(\"=\"*60)\n\n# Verify GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# === STEP 1: Create Training Data ===\nprint(\"\\n[1/4] Preparing training data...\")\n\n# Load seeds and create biomedical QA training prompts\nseeds_file = Path(CONFIG['seeds_file'])\ncorpus_file = Path(CONFIG['corpus_file'])\n\ntraining_examples = []\nwith open(seeds_file) as f:\n    seeds = [json.loads(line) for line in f]\n\nprint(f\"  Loaded {len(seeds)} seed papers\")\n\n# Create multi-hop biomedical question prompts\n# The proposer learns to generate research questions from paper contexts\nsystem_prompt = \"\"\"You are a biomedical research assistant. Given a scientific paper abstract, generate a challenging multi-hop research question that would require searching PubMed to answer. The question should connect multiple biomedical concepts and require reasoning across papers.\n\nFormat your response as a single research question.\"\"\"\n\nfor i, seed in enumerate(seeds):\n    abstract = seed.get('abstract', seed.get('text', ''))[:1000]  # Truncate long abstracts\n    title = seed.get('title', '')\n    \n    # Create training example: paper context -> research question\n    prompt = f\"Based on this paper about '{title}':\\n\\n{abstract}\\n\\nGenerate a research question:\"\n    \n    # For initial training, we use simple self-supervision\n    # The model learns the format of good biomedical questions\n    training_examples.append({\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        \"pmid\": seed.get('pmid', str(i))\n    })\n\nprint(f\"  Created {len(training_examples)} training examples\")\n\n# === STEP 2: Load Model and Tokenizer ===\nprint(\"\\n[2/4] Loading model...\")\n\nmodel_name = CONFIG['model_name']\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    use_cache=False  # Required for gradient checkpointing\n)\nmodel.gradient_checkpointing_enable()\n\nprint(f\"  Model loaded: {model_name}\")\nprint(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n\n# === STEP 3: Prepare Dataset ===\nprint(\"\\n[3/4] Tokenizing dataset...\")\n\ndef tokenize_function(examples):\n    # Apply chat template\n    texts = []\n    for msgs in examples[\"messages\"]:\n        text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n        texts.append(text)\n    \n    tokenized = tokenizer(\n        texts,\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n    return tokenized\n\ndataset = Dataset.from_list(training_examples)\ndataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=32,\n    remove_columns=[\"messages\", \"pmid\"],\n    desc=\"Tokenizing\"\n)\ndataset.set_format(\"torch\")\n\nprint(f\"  Dataset size: {len(dataset)}\")\n\n# === STEP 4: Training ===\nprint(\"\\n[4/4] Starting training...\")\nprint(f\"  Steps: {CONFIG['max_steps']}\")\nprint(f\"  Batch size: {CONFIG['batch_size']}\")\nprint(f\"  Learning rate: {CONFIG['learning_rate']}\")\n\n# Simple training loop for Colab (avoids TRL/veRL complexity)\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# Dataloader\ntrain_dataloader = DataLoader(\n    dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    drop_last=True\n)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n\n# Scheduler\nnum_training_steps = min(CONFIG['max_steps'], len(train_dataloader) * CONFIG['gradient_accumulation'])\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.03 * num_training_steps),\n    num_training_steps=num_training_steps\n)\n\n# Training loop\nmodel.train()\nglobal_step = 0\ntotal_loss = 0\ncheckpoint_dir = Path(CONFIG['temp_checkpoint'])\n\nprogress_bar = tqdm(total=CONFIG['max_steps'], desc=\"Training\")\n\nfor epoch in range(10):  # Max epochs (will break early based on steps)\n    for batch_idx, batch in enumerate(train_dataloader):\n        # Move to GPU\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss / CONFIG['gradient_accumulation']\n        \n        # Backward pass\n        loss.backward()\n        total_loss += loss.item()\n        \n        # Update weights every gradient_accumulation steps\n        if (batch_idx + 1) % CONFIG['gradient_accumulation'] == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n            global_step += 1\n            avg_loss = total_loss * CONFIG['gradient_accumulation']\n            total_loss = 0\n            \n            progress_bar.update(1)\n            progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n            \n            # Save checkpoint\n            if global_step % CONFIG['save_freq'] == 0:\n                ckpt_path = checkpoint_dir / f\"step_{global_step}\"\n                ckpt_path.mkdir(parents=True, exist_ok=True)\n                model.save_pretrained(ckpt_path)\n                tokenizer.save_pretrained(ckpt_path)\n                print(f\"\\n  Saved checkpoint: {ckpt_path}\")\n            \n            if global_step >= CONFIG['max_steps']:\n                break\n    \n    if global_step >= CONFIG['max_steps']:\n        break\n\nprogress_bar.close()\n\n# === STEP 5: Save Final Model ===\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)\n\n# Save to Google Drive\nfinal_path = Path(CONFIG['final_checkpoint']) / \"proposer_final\"\nfinal_path.mkdir(parents=True, exist_ok=True)\nmodel.save_pretrained(final_path)\ntokenizer.save_pretrained(final_path)\n\nprint(f\"\\nFinal model saved to: {final_path}\")\nprint(f\"Total steps: {global_step}\")\n\n# Free memory\ndel model, optimizer\ntorch.cuda.empty_cache()\nprint(\"\\n[OK] Training complete! Model saved to Google Drive.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 10: Check Storage & Results\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    path = Path(path)\n",
    "    if path.exists():\n",
    "        for f in path.rglob('*'):\n",
    "            if f.is_file():\n",
    "                total += f.stat().st_size\n",
    "    return total / (1024**3)  # GB\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STORAGE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "drive_base = Path('/content/drive/MyDrive/drzero_lite')\n",
    "local_base = Path('/content/drzero_temp')\n",
    "\n",
    "print(\"\\nGoogle Drive (persistent):\")\n",
    "corpus_size = get_dir_size(drive_base / 'corpus')\n",
    "ckpt_size = get_dir_size(drive_base / 'final_checkpoint')\n",
    "print(f\"  Corpus: {corpus_size:.2f} GB\")\n",
    "print(f\"  Final checkpoint: {ckpt_size:.2f} GB\")\n",
    "print(f\"  TOTAL: {corpus_size + ckpt_size:.2f} GB / 15 GB\")\n",
    "\n",
    "print(\"\\nLocal temp (will be deleted):\")\n",
    "temp_size = get_dir_size(local_base)\n",
    "print(f\"  Temp files: {temp_size:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "total_drive = corpus_size + ckpt_size\n",
    "if total_drive < 15:\n",
    "    print(f\"[OK] Within 15 GB Drive limit ({total_drive:.1f} GB used)\")\n",
    "else:\n",
    "    print(f\"[WARNING] Exceeds 15 GB limit ({total_drive:.1f} GB used)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# List checkpoints\n",
    "print(\"\\nSaved checkpoints:\")\n",
    "ckpt_dir = drive_base / 'final_checkpoint'\n",
    "if ckpt_dir.exists():\n",
    "    for item in sorted(ckpt_dir.iterdir()):\n",
    "        print(f\"  {item.name}\")\n",
    "else:\n",
    "    print(\"  None yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Complete!\n",
    "\n",
    "## What You Have\n",
    "\n",
    "- PubMed corpus (10K papers)\n",
    "- PubMedBERT search index\n",
    "- Trained proposer model\n",
    "- All saved to Google Drive (<15 GB)\n",
    "\n",
    "## Download Your Model\n",
    "\n",
    "```python\n",
    "# Zip and download\n",
    "!zip -r model.zip /content/drive/MyDrive/drzero_lite/final_checkpoint\n",
    "from google.colab import files\n",
    "files.download('model.zip')\n",
    "```\n",
    "\n",
    "## Test Your Model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"/content/drive/MyDrive/drzero_lite/final_checkpoint/step_100\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "# Generate\n",
    "prompt = \"Generate a research question about breast cancer and TP53:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Evaluate on PubMedQA benchmark\n",
    "2. Generate biomedical QA pairs\n",
    "3. Fine-tune solver (optional - adds ~5 hours)\n",
    "4. Iterate for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}