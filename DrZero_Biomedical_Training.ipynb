{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Zero Biomedical Training on Google Colab\n",
    "\n",
    "This notebook provides a complete pipeline for training Dr. Zero on biomedical literature (PubMed) using Google Colab Pro+ with A100 GPU.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Training Pipeline:**\n",
    "1. Setup environment and download PubMed corpus\n",
    "2. Build PubMedBERT search index\n",
    "3. Train Iteration 1 (Proposer + Solver)\n",
    "4. Train Iteration 2 (with improved solver)\n",
    "5. Train Iteration 3 (final models)\n",
    "6. Evaluate on biomedical QA benchmarks\n",
    "\n",
    "**Expected Runtime:** 30-40 hours on A100 GPU\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro/Pro+ (for A100 GPU and long runtime)\n",
    "- ~50 GB Google Drive storage\n",
    "- Weights & Biases account (for logging)\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "1. **Set runtime to A100 GPU:**\n   - Runtime ‚Üí Change runtime type ‚Üí A100 GPU\n",
    "2. **Get W&B API key:**\n",
    "   - Sign up at wandb.ai\n",
    "   - Get API key from wandb.ai/authorize\n",
    "3. **Have your email ready** (required for NCBI PubMed API)\n",
    "\n",
    "## Execution Instructions\n",
    "\n",
    "Run cells in order. The notebook includes:\n",
    "- ‚úÖ Automatic checkpointing to Google Drive\n",
    "- üîÑ Auto-resume from disconnections\n",
    "- üìä Progress monitoring\n",
    "- üõ°Ô∏è Error handling and recovery\n",
    "\n",
    "**Do NOT skip cells** - they build on each other.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Environment Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Mount Google Drive and Setup Directories\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CELL 1: Mounting Google Drive & Creating Directories\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# Create directory structure in Google Drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory in Google Drive\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_biomedical')\n",
    "DRIVE_BASE.mkdir(exist_ok=True)\n",
    "\n",
    "# Subdirectories\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus' / 'pubmed'\n",
    "CHECKPOINT_DIR = DRIVE_BASE / 'checkpoints'\n",
    "DATA_DIR = DRIVE_BASE / 'data' / 'biomedical'\n",
    "LOGS_DIR = DRIVE_BASE / 'logs'\n",
    "OUTPUTS_DIR = DRIVE_BASE / 'outputs'\n",
    "\n",
    "for dir_path in [CORPUS_DIR, CHECKPOINT_DIR, DATA_DIR, LOGS_DIR, OUTPUTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úì Created: {dir_path}\")\n",
    "\n",
    "# Create local directories (faster access during training)\n",
    "LOCAL_BASE = Path('/content/drzero_local')\n",
    "LOCAL_CHECKPOINT = LOCAL_BASE / 'checkpoints'\n",
    "LOCAL_DATA = LOCAL_BASE / 'data'\n",
    "\n",
    "for dir_path in [LOCAL_CHECKPOINT, LOCAL_DATA]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úì Created local: {dir_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Directory structure ready!\")\n",
    "print(f\"   Drive base: {DRIVE_BASE}\")\n",
    "print(f\"   Local base: {LOCAL_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CELL 2: Installing Dependencies\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package, quiet=True):\n",
    "    \"\"\"Install a package with pip.\"\"\"\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
    "    if quiet:\n",
    "        cmd.append(\"-q\")\n",
    "    cmd.append(package)\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Core dependencies\n",
    "print(\"\\nüì¶ Installing core packages...\")\n",
    "core_packages = [\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\",\n",
    "    \"datasets\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-gpu\",\n",
    "    \"biopython\",\n",
    "    \"wandb\",\n",
    "    \"tqdm\",\n",
    "    \"psutil\",\n",
    "]\n",
    "\n",
    "for pkg in core_packages:\n",
    "    try:\n",
    "        install_package(pkg)\n",
    "        print(f\"  ‚úì {pkg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Failed: {pkg} - {e}\")\n",
    "\n",
    "# Install SGLang for serving\n",
    "print(\"\\nüì¶ Installing SGLang...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sglang[all]\"])\n",
    "    print(\"  ‚úì sglang\")\n",
    "except:\n",
    "    print(\"  ‚ö†Ô∏è SGLang installation failed, will try alternative method\")\n",
    "\n",
    "# Install veRL from source\n",
    "print(\"\\nüì¶ Installing veRL framework...\")\n",
    "if not os.path.exists('/content/verl'):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"https://github.com/volcengine/verl.git\", \"/content/verl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/content/verl\"])\n",
    "    print(\"  ‚úì veRL installed from source\")\n",
    "else:\n",
    "    print(\"  ‚úì veRL already installed\")\n",
    "\n",
    "# Verify installations\n",
    "print(\"\\nüîç Verifying installations...\")\n",
    "import torch\n",
    "import transformers\n",
    "print(f\"  ‚úì PyTorch: {torch.__version__}\")\n",
    "print(f\"  ‚úì Transformers: {transformers.__version__}\")\n",
    "print(f\"  ‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  ‚úì GPU: {gpu_name}\")\n",
    "    print(f\"  ‚úì GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if \"A100\" not in gpu_name:\n",
    "        print(f\"  ‚ö†Ô∏è WARNING: Expected A100 GPU, got {gpu_name}\")\n",
    "        print(f\"     Training may be slower or run out of memory\")\n",
    "else:\n",
    "    print(\"  ‚ùå ERROR: No GPU detected!\")\n",
    "    print(\"     Go to Runtime -> Change runtime type -> Select A100 GPU\")\n",
    "    raise RuntimeError(\"GPU required for training\")\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Clone Dr. Zero and Setup Biomedical Module\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CELL 3: Setting Up Dr. Zero Repository\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Clone Dr. Zero if not exists\n",
    "DRZERO_DIR = Path('/content/drzero')\n",
    "if not DRZERO_DIR.exists():\n",
    "    print(\"\\nüì• Cloning Dr. Zero repository...\")\n",
    "    subprocess.check_call([\n",
    "        \"git\", \"clone\", \n",
    "        \"https://github.com/facebookresearch/drzero.git\",\n",
    "        str(DRZERO_DIR)\n",
    "    ])\n",
    "    print(\"  ‚úì Dr. Zero cloned\")\n",
    "else:\n",
    "    print(\"\\n‚úì Dr. Zero already cloned\")\n",
    "\n",
    "# Change to drzero directory\n",
    "os.chdir(DRZERO_DIR)\n",
    "print(f\"\\nüìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Copy biomedical module (assuming it's uploaded to Colab or Drive)\n",
    "print(\"\\nüìã Setting up biomedical module...\")\n",
    "\n",
    "# Check if biomedical module exists in current directory or Drive\n",
    "BIOMEDICAL_SOURCE = None\n",
    "for search_path in [\n",
    "    Path('/content/biomedical'),  # If uploaded directly\n",
    "    DRIVE_BASE.parent / 'biomedical',  # If in Drive\n",
    "    Path.cwd() / 'biomedical'  # If already copied\n",
    "]:\n",
    "    if search_path.exists():\n",
    "        BIOMEDICAL_SOURCE = search_path\n",
    "        break\n",
    "\n",
    "if BIOMEDICAL_SOURCE and BIOMEDICAL_SOURCE != DRZERO_DIR / 'biomedical':\n",
    "    shutil.copytree(BIOMEDICAL_SOURCE, DRZERO_DIR / 'biomedical', dirs_exist_ok=True)\n",
    "    print(f\"  ‚úì Copied biomedical module from {BIOMEDICAL_SOURCE}\")\n",
    "elif (DRZERO_DIR / 'biomedical').exists():\n",
    "    print(\"  ‚úì Biomedical module already in place\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Biomedical module not found!\")\n",
    "    print(\"     Please upload the 'biomedical/' folder to:\")\n",
    "    print(f\"     - /content/biomedical/ OR\")\n",
    "    print(f\"     - {DRIVE_BASE.parent}/biomedical/\")\n",
    "    raise FileNotFoundError(\"Biomedical module required\")\n",
    "\n",
    "# Copy helper files\n",
    "print(\"\\nüìã Copying helper files...\")\n",
    "helper_files = [\n",
    "    'colab_helpers.py',\n",
    "    'colab_config.yaml'\n",
    "]\n",
    "\n",
    "for filename in helper_files:\n",
    "    # Check multiple locations\n",
    "    for source in [Path('/content') / filename, DRIVE_BASE.parent / filename]:\n",
    "        if source.exists():\n",
    "            shutil.copy(source, DRZERO_DIR / filename)\n",
    "            print(f\"  ‚úì Copied {filename}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è {filename} not found (will create if needed)\")\n",
    "\n",
    "# Verify biomedical module\n",
    "print(\"\\nüîç Verifying biomedical module...\")\n",
    "try:\n",
    "    from biomedical import (\n",
    "        PubMedCorpusManager,\n",
    "        BiomedicalValidator,\n",
    "        BiomedicalRetrieverServer,\n",
    "        BiomedicalPrompts,\n",
    "        BiomedicalRewardCalculator,\n",
    "        BiomedicalDatasets\n",
    "    )\n",
    "    print(\"  ‚úì All biomedical components imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå Import error: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Configuration & Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CELL 4: Configuration\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# User inputs (MODIFY THESE)\n",
    "import getpass\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Setting up configuration...\\n\")\n",
    "\n",
    "# NCBI Email (required for PubMed API)\n",
    "NCBI_EMAIL = \"ssa163@case.edu\"  # CHANGE THIS to your email\n",
    "print(f\"üìß NCBI Email: {NCBI_EMAIL}\")\n",
    "\n",
    "# Weights & Biases API key\n",
    "print(\"\\nüîë Weights & Biases Setup:\")\n",
    "print(\"   Get your API key from: https://wandb.ai/authorize\")\n",
    "WANDB_API_KEY = getpass.getpass(\"Enter W&B API key (hidden): \")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n",
    "    import wandb\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    print(\"  ‚úì W&B configured\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è No W&B key provided - logging will be disabled\")\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    \n",
    "    # Data\n",
    "    'corpus_size': 50000,  # Number of PubMed papers to download\n",
    "    'training_seeds': 2000,  # Number of seed documents\n",
    "    'pubmed_query': '(breast cancer OR lung cancer OR drug resistance) AND (gene OR protein OR pathway)',\n",
    "    'date_range': ('2020/01/01', '2024/12/31'),\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation': 4,\n",
    "    'learning_rate': 1e-6,\n",
    "    'max_steps_per_iteration': 200,  # Steps per iteration (adjust based on data size)\n",
    "    \n",
    "    # Paths\n",
    "    'corpus_path': str(CORPUS_DIR),\n",
    "    'checkpoint_dir': str(CHECKPOINT_DIR),\n",
    "    'data_dir': str(DATA_DIR),\n",
    "    'logs_dir': str(LOGS_DIR),\n",
    "    \n",
    "    # Servers\n",
    "    'retrieval_port': 8000,\n",
    "    'solver_port': 8001,\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Download PubMed Corpus\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CELL 5: Downloading PubMed Corpus\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from biomedical import PubMedCorpusManager\n",
    "\n",
    "# Check if corpus already exists\n",
    "corpus_file = Path(CONFIG['corpus_path']) / 'pubmed-corpus.jsonl'\n",
    "\n",
    "if corpus_file.exists():\n",
    "    print(f\"\\n‚úì Corpus already exists: {corpus_file}\")\n",
    "    \n",
    "    # Count existing papers\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        n_existing = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"  Papers in corpus: {n_existing}\")\n",
    "    \n",
    "    if n_existing >= CONFIG['corpus_size']:\n",
    "        print(\"  Skipping download (sufficient papers already downloaded)\")\n",
    "    else:\n",
    "        print(f\"  Need to download {CONFIG['corpus_size'] - n_existing} more papers\")\n",
    "        download_corpus = True\n",
    "else:\n",
    "    print(\"\\nüì• Downloading PubMed corpus...\")\n",
    "    print(f\"   Query: {CONFIG['pubmed_query']}\")\n",
    "    print(f\"   Max papers: {CONFIG['corpus_size']}\")\n",
    "    print(f\"   Date range: {CONFIG['date_range']}\")\n",
    "    print(\"\\n‚è±Ô∏è This will take 30-60 minutes...\")\n",
    "    \n",
    "    download_corpus = True\n",
    "\n",
    "if download_corpus:\n",
    "    # Initialize corpus manager\n",
    "    manager = PubMedCorpusManager(\n",
    "        save_path=CONFIG['corpus_path'],\n",
    "        email=NCBI_EMAIL\n",
    "    )\n",
    "    \n",
    "    # Download\n",
    "    articles = manager.download_pubmed_abstracts(\n",
    "        query=CONFIG['pubmed_query'],\n",
    "        max_results=CONFIG['corpus_size'],\n",
    "        date_range=CONFIG['date_range']\n",
    "    )\n",
    "    \n",
    "    if articles:\n",
    "        # Save corpus\n",
    "        manager.save_corpus(articles)\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = manager.get_corpus_statistics()\n",
    "        print(\"\\nüìä Corpus Statistics:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Downloaded {len(articles)} papers!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Download failed - check your internet and NCBI email\")\n",
    "        raise RuntimeError(\"Corpus download failed\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Using existing corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Checkpoint: Corpus Downloaded\n",
    "\n",
    "At this point, you have:\n",
    "- ‚úÖ PubMed corpus downloaded to Google Drive\n",
    "- ‚úÖ Environment fully configured\n",
    "\n",
    "**If you need to stop here:**\n",
    "- Your corpus is safely stored in Google Drive\n",
    "- You can resume from the next cell later\n",
    "\n",
    "**To continue:** Run the next cells to build the search index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a comprehensive Jupyter notebook, but due to size constraints, I'm providing the first 5 critical cells. The complete notebook would continue with:\n",
    "\n",
    "- Cells 6-7: Build FAISS index\n",
    "- Cells 8-9: Prepare training data\n",
    "- Cells 10-15: Iteration 1 training\n",
    "- Cells 16-21: Iteration 2 training\n",
    "- Cells 22-27: Iteration 3 training\n",
    "- Cells 28-30: Evaluation\n",
    "\n",
    "Would you like me to:\n",
    "1. Continue with the remaining cells in the notebook?\n",
    "2. Create a simplified version?\n",
    "3. Focus on specific sections?\n",
    "\n",
    "Let me know how you'd like to proceed!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
