{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Zero Biomedical Training on Google Colab\n",
    "\n",
    "This notebook provides a complete pipeline for training Dr. Zero on biomedical literature (PubMed) using Google Colab Pro+ with A100 GPU.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Training Pipeline:**\n",
    "1. Setup environment and download PubMed corpus\n",
    "2. Build PubMedBERT search index\n",
    "3. Train Iteration 1 (Proposer + Solver)\n",
    "4. Train Iteration 2 (with improved solver)\n",
    "5. Train Iteration 3 (final models)\n",
    "6. Evaluate on biomedical QA benchmarks\n",
    "\n",
    "**Expected Runtime:** 30-40 hours on A100 GPU\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro/Pro+ (for A100 GPU and long runtime)\n",
    "- ~50 GB Google Drive storage\n",
    "- Weights & Biases account (for logging)\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "1. **Set runtime to A100 GPU:**\n   - Runtime ‚Üí Change runtime type ‚Üí A100 GPU\n",
    "2. **Get W&B API key:**\n",
    "   - Sign up at wandb.ai\n",
    "   - Get API key from wandb.ai/authorize\n",
    "3. **Have your email ready** (required for NCBI PubMed API)\n",
    "\n",
    "## Execution Instructions\n",
    "\n",
    "Run cells in order. The notebook includes:\n",
    "- ‚úÖ Automatic checkpointing to Google Drive\n",
    "- üîÑ Auto-resume from disconnections\n",
    "- üìä Progress monitoring\n",
    "- üõ°Ô∏è Error handling and recovery\n",
    "\n",
    "**Do NOT skip cells** - they build on each other.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Environment Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Mount Both Google Drives (Dual-Account Setup)\n\nprint(\"=\"*80)\nprint(\"CELL 1: Dual Google Drive Setup\")\nprint(\"=\"*80)\n\nfrom google.colab import drive\nfrom pathlib import Path\nimport os\n\nprint(\"\\nüìö You have two Google accounts:\")\nprint(\"   Account A (Colab Pro): A100 GPU access, 15GB storage\")\nprint(\"   Account B (Storage): 80GB storage\")\nprint(\"\\nWe'll mount BOTH drives to use Account A for compute\")\nprint(\"and Account B for storage!\")\n\n# Mount Account A (Colab Pro) - for small files\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 1: Mount Account A (Colab Pro Account)\")\nprint(\"=\"*80)\nprint(\"\\nüìå Click the link below and authenticate with your COLAB PRO account\")\nprint(\"   (The account you're using to run this notebook)\\n\")\n\ndrive.mount('/content/drive_pro', force_remount=False)\nprint(\"\\n‚úÖ Account A (Colab Pro) mounted successfully!\")\n\n# Mount Account B (80GB Storage) - for large files  \nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: Mount Account B (80GB Storage Account)\")\nprint(\"=\"*80)\nprint(\"\\nüìå Click the link below and:\")\nprint(\"   1. Click 'Use another account'\")\nprint(\"   2. Sign in with your 80GB STORAGE account\")\nprint(\"   3. Authorize access\\n\")\n\ndrive.mount('/content/drive_storage', force_remount=False)\nprint(\"\\n‚úÖ Account B (Storage) mounted successfully!\")\n\n# Verify both mounts\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 3: Verify Both Drives\")\nprint(\"=\"*80)\n\ntry:\n    pro_contents = os.listdir('/content/drive_pro/MyDrive')\n    storage_contents = os.listdir('/content/drive_storage/MyDrive')\n    \n    print(f\"\\n‚úÖ Pro Drive (Account A): {len(pro_contents)} items\")\n    print(f\"   Path: /content/drive_pro/MyDrive/\")\n    print(f\"   Sample: {pro_contents[:3] if pro_contents else '(empty)'}\")\n    \n    print(f\"\\n‚úÖ Storage Drive (Account B): {len(storage_contents)} items\")\n    print(f\"   Path: /content/drive_storage/MyDrive/\")\n    print(f\"   Sample: {storage_contents[:3] if storage_contents else '(empty)'}\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing drives: {e}\")\n    print(\"   Please re-run this cell and ensure both accounts are authenticated\")\n    raise\n\n# Create directory structure on BOTH drives\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 4: Creating Directory Structure\")\nprint(\"=\"*80)\n\n# Account A (Pro) - Small files only\nPRO_BASE = Path('/content/drive_pro/MyDrive/drzero_biomedical')\nLOGS_DIR = PRO_BASE / 'logs'\nCONFIG_DIR = PRO_BASE / 'configs'\n\nfor dir_path in [PRO_BASE, LOGS_DIR, CONFIG_DIR]:\n    dir_path.mkdir(parents=True, exist_ok=True)\n    print(f\"‚úì Created (Pro Drive): {dir_path}\")\n\n# Account B (Storage) - Large files\nSTORAGE_BASE = Path('/content/drive_storage/MyDrive/drzero_biomedical')\nCORPUS_DIR = STORAGE_BASE / 'corpus' / 'pubmed'\nCHECKPOINT_DIR = STORAGE_BASE / 'checkpoints'\nDATA_DIR = STORAGE_BASE / 'data' / 'biomedical'\nOUTPUTS_DIR = STORAGE_BASE / 'outputs'\n\nfor dir_path in [STORAGE_BASE, CORPUS_DIR, CHECKPOINT_DIR, DATA_DIR, OUTPUTS_DIR]:\n    dir_path.mkdir(parents=True, exist_ok=True)\n    print(f\"‚úì Created (Storage Drive): {dir_path}\")\n\n# Create local temp directories (fastest access during training)\nLOCAL_BASE = Path('/content/drzero_local')\nLOCAL_CHECKPOINT = LOCAL_BASE / 'checkpoints'\nLOCAL_DATA = LOCAL_BASE / 'data'\n\nfor dir_path in [LOCAL_BASE, LOCAL_CHECKPOINT, LOCAL_DATA]:\n    dir_path.mkdir(parents=True, exist_ok=True)\n    print(f\"‚úì Created (Local): {dir_path}\")\n\n# Print storage allocation summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä STORAGE ALLOCATION SUMMARY\")\nprint(\"=\"*80)\n\nprint(\"\\nüìÅ Account A (Colab Pro - 15GB):\")\nprint(f\"   Logs: {LOGS_DIR}\")\nprint(f\"   Configs: {CONFIG_DIR}\")\nprint(f\"   Expected usage: <1 GB\")\n\nprint(\"\\nüìÅ Account B (Storage - 80GB):\")\nprint(f\"   Corpus: {CORPUS_DIR} (~10 GB)\")\nprint(f\"   Checkpoints: {CHECKPOINT_DIR} (~30 GB)\")\nprint(f\"   Data: {DATA_DIR} (~5 GB)\")\nprint(f\"   Outputs: {OUTPUTS_DIR} (~2 GB)\")\nprint(f\"   Expected usage: ~47 GB\")\n\nprint(\"\\nüí° Local temp (Colab VM - fast but not persistent):\")\nprint(f\"   {LOCAL_BASE}\")\nprint(f\"   Used for: Working files during training\")\n\nprint(\"\\n‚úÖ Dual-drive setup complete!\")\nprint(\"   Both accounts accessible in this session\")\nprint(\"   Large files ‚Üí Storage account\")\nprint(\"   Small files ‚Üí Pro account\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CELL 2: Installing Dependencies\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package, quiet=True):\n",
    "    \"\"\"Install a package with pip.\"\"\"\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
    "    if quiet:\n",
    "        cmd.append(\"-q\")\n",
    "    cmd.append(package)\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Core dependencies\n",
    "print(\"\\nüì¶ Installing core packages...\")\n",
    "core_packages = [\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\",\n",
    "    \"datasets\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-gpu\",\n",
    "    \"biopython\",\n",
    "    \"wandb\",\n",
    "    \"tqdm\",\n",
    "    \"psutil\",\n",
    "]\n",
    "\n",
    "for pkg in core_packages:\n",
    "    try:\n",
    "        install_package(pkg)\n",
    "        print(f\"  ‚úì {pkg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Failed: {pkg} - {e}\")\n",
    "\n",
    "# Install SGLang for serving\n",
    "print(\"\\nüì¶ Installing SGLang...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sglang[all]\"])\n",
    "    print(\"  ‚úì sglang\")\n",
    "except:\n",
    "    print(\"  ‚ö†Ô∏è SGLang installation failed, will try alternative method\")\n",
    "\n",
    "# Install veRL from source\n",
    "print(\"\\nüì¶ Installing veRL framework...\")\n",
    "if not os.path.exists('/content/verl'):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"https://github.com/volcengine/verl.git\", \"/content/verl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/content/verl\"])\n",
    "    print(\"  ‚úì veRL installed from source\")\n",
    "else:\n",
    "    print(\"  ‚úì veRL already installed\")\n",
    "\n",
    "# Verify installations\n",
    "print(\"\\nüîç Verifying installations...\")\n",
    "import torch\n",
    "import transformers\n",
    "print(f\"  ‚úì PyTorch: {torch.__version__}\")\n",
    "print(f\"  ‚úì Transformers: {transformers.__version__}\")\n",
    "print(f\"  ‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  ‚úì GPU: {gpu_name}\")\n",
    "    print(f\"  ‚úì GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if \"A100\" not in gpu_name:\n",
    "        print(f\"  ‚ö†Ô∏è WARNING: Expected A100 GPU, got {gpu_name}\")\n",
    "        print(f\"     Training may be slower or run out of memory\")\n",
    "else:\n",
    "    print(\"  ‚ùå ERROR: No GPU detected!\")\n",
    "    print(\"     Go to Runtime -> Change runtime type -> Select A100 GPU\")\n",
    "    raise RuntimeError(\"GPU required for training\")\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Clone DrPubMedZero Repository (Much Simpler!)\n\nprint(\"=\"*80)\nprint(\"CELL 3: Cloning DrPubMedZero Repository\")\nprint(\"=\"*80)\n\nimport os\nimport subprocess\nfrom pathlib import Path\nimport sys\n\n# Clone your DrPubMedZero repository (contains everything!)\nREPO_DIR = Path('/content/DrPubMedZero')\n\nif not REPO_DIR.exists():\n    print(\"\\nüì• Cloning DrPubMedZero repository from GitHub...\")\n    subprocess.check_call([\n        \"git\", \"clone\", \n        \"https://github.com/ShivaAyyar/DrPubMedZero.git\",\n        str(REPO_DIR)\n    ])\n    print(\"  ‚úì Repository cloned\")\nelse:\n    print(\"\\n‚úì Repository already exists\")\n    print(\"  Pulling latest changes...\")\n    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"pull\"])\n    print(\"  ‚úì Up to date\")\n\n# Change to repository directory\nos.chdir(REPO_DIR)\nprint(f\"\\nüìÇ Working directory: {os.getcwd()}\")\n\n# Verify all required files exist\nprint(\"\\nüîç Verifying repository contents...\")\n\nrequired_items = [\n    'biomedical/',\n    'colab_helpers.py',\n    'colab_config.yaml',\n    'config/',\n    'scripts/download.py',\n    'iter1_challenger_biomed.sh',\n    'iter2_challenger_biomed.sh',\n    'iter3_challenger_biomed.sh',\n    'requirements.txt'\n]\n\nall_present = True\nfor item in required_items:\n    path = Path(item)\n    if path.exists():\n        print(f\"  ‚úì {item}\")\n    else:\n        print(f\"  ‚ùå Missing: {item}\")\n        all_present = False\n\nif not all_present:\n    print(\"\\n‚ö†Ô∏è Some files are missing. Ensure your repository is up to date.\")\n    raise FileNotFoundError(\"Required files missing from repository\")\n\n# Verify biomedical module imports\nprint(\"\\nüîç Verifying biomedical module...\")\nsys.path.insert(0, str(REPO_DIR))  # Add to Python path\n\ntry:\n    from biomedical import (\n        PubMedCorpusManager,\n        BiomedicalValidator,\n        BiomedicalRetrieverServer,\n        BiomedicalPrompts,\n        BiomedicalRewardCalculator,\n        BiomedicalDatasets,\n        setup_for_colab\n    )\n    print(\"  ‚úì All biomedical components imported successfully\")\n    \n    # Run Colab setup\n    print(\"\\nüîß Configuring for Colab environment...\")\n    if setup_for_colab():\n        print(\"  ‚úì Colab environment configured\")\n    \nexcept ImportError as e:\n    print(f\"  ‚ùå Import error: {e}\")\n    print(\"\\nüì¶ Installing missing dependencies from requirements.txt...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"requirements.txt\"])\n    print(\"  ‚úì Dependencies installed\")\n    print(\"\\n‚ö†Ô∏è Please re-run this cell to complete setup\")\n\nprint(\"\\n‚úÖ Repository setup complete!\")\nprint(f\"   All code is in: {REPO_DIR}\")\nprint(f\"\\nüí° TIP: Your repository is now cloned. All biomedical\")\nprint(f\"   modules, configs, and training scripts are ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Configuration & Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Configuration (Dual-Drive Aware)\n\nprint(\"=\"*80)\nprint(\"CELL 4: Configuration\")\nprint(\"=\"*80)\n\n# User inputs (MODIFY THESE)\nimport getpass\n\nprint(\"\\n‚öôÔ∏è Setting up configuration...\\n\")\n\n# NCBI Email (required for PubMed API)\nNCBI_EMAIL = \"ssa163@case.edu\"  # Your email\nprint(f\"üìß NCBI Email: {NCBI_EMAIL}\")\n\n# Weights & Biases API key\nprint(\"\\nüîë Weights & Biases Setup:\")\nprint(\"   Get your API key from: https://wandb.ai/authorize\")\nWANDB_API_KEY = getpass.getpass(\"Enter W&B API key (hidden): \")\n\nif WANDB_API_KEY:\n    os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n    import wandb\n    wandb.login(key=WANDB_API_KEY)\n    print(\"  ‚úì W&B configured\")\nelse:\n    print(\"  ‚ö†Ô∏è No W&B key provided - logging will be disabled\")\n    os.environ['WANDB_MODE'] = 'disabled'\n\n# Training configuration with DUAL-DRIVE PATHS\nprint(\"\\nüìÅ Configuring dual-drive storage paths...\")\n\nCONFIG = {\n    # Model\n    'model_name': 'Qwen/Qwen2.5-3B-Instruct',\n    \n    # Data\n    'corpus_size': 50000,  # Number of PubMed papers to download\n    'training_seeds': 2000,  # Number of seed documents\n    'pubmed_query': '(breast cancer OR lung cancer OR drug resistance) AND (gene OR protein OR pathway)',\n    'date_range': ('2020/01/01', '2024/12/31'),\n    \n    # Training\n    'batch_size': 64,\n    'gradient_accumulation': 4,\n    'learning_rate': 1e-6,\n    'max_steps_per_iteration': 200,  # Steps per iteration (adjust based on data size)\n    \n    # Paths - DUAL-DRIVE SETUP\n    # Large files ‚Üí Storage account (80GB)\n    'corpus_path': str(CORPUS_DIR),  # Account B\n    'checkpoint_dir': str(CHECKPOINT_DIR),  # Account B\n    'data_dir': str(DATA_DIR),  # Account B\n    'outputs_dir': str(OUTPUTS_DIR),  # Account B\n    \n    # Small files ‚Üí Pro account (15GB)\n    'logs_dir': str(LOGS_DIR),  # Account A\n    'config_dir': str(CONFIG_DIR),  # Account A\n    \n    # Servers\n    'retrieval_port': 8000,\n    'solver_port': 8001,\n}\n\nprint(\"\\nüìã Training Configuration:\")\nprint(\"\\nüîπ Model & Training:\")\nfor key in ['model_name', 'batch_size', 'gradient_accumulation', 'learning_rate']:\n    print(f\"   {key}: {CONFIG[key]}\")\n\nprint(\"\\nüîπ Data:\")\nfor key in ['corpus_size', 'training_seeds', 'pubmed_query']:\n    value = CONFIG[key]\n    if isinstance(value, str) and len(value) > 60:\n        value = value[:57] + \"...\"\n    print(f\"   {key}: {value}\")\n\nprint(\"\\nüîπ Storage Paths (Dual-Drive):\")\nprint(f\"   üìÅ Account A (Pro - 15GB):\")\nprint(f\"      logs_dir: {CONFIG['logs_dir']}\")\nprint(f\"      config_dir: {CONFIG['config_dir']}\")\nprint(f\"\\n   üìÅ Account B (Storage - 80GB):\")\nprint(f\"      corpus_path: {CONFIG['corpus_path']}\")\nprint(f\"      checkpoint_dir: {CONFIG['checkpoint_dir']}\")\nprint(f\"      data_dir: {CONFIG['data_dir']}\")\nprint(f\"      outputs_dir: {CONFIG['outputs_dir']}\")\n\n# Verify paths exist\nprint(\"\\nüîç Verifying all directories...\")\nall_paths_exist = True\nfor key, path in CONFIG.items():\n    if '_dir' in key or '_path' in key:\n        if not Path(path).exists():\n            print(f\"   ‚ö†Ô∏è Creating: {path}\")\n            Path(path).mkdir(parents=True, exist_ok=True)\n        else:\n            print(f\"   ‚úì {key}: exists\")\n\nprint(\"\\n‚úÖ Configuration complete!\")\nprint(\"   Dual-drive setup verified\")\nprint(\"   All paths ready for training\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Download PubMed Corpus\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CELL 5: Downloading PubMed Corpus\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from biomedical import PubMedCorpusManager\n",
    "\n",
    "# Check if corpus already exists\n",
    "corpus_file = Path(CONFIG['corpus_path']) / 'pubmed-corpus.jsonl'\n",
    "\n",
    "if corpus_file.exists():\n",
    "    print(f\"\\n‚úì Corpus already exists: {corpus_file}\")\n",
    "    \n",
    "    # Count existing papers\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        n_existing = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"  Papers in corpus: {n_existing}\")\n",
    "    \n",
    "    if n_existing >= CONFIG['corpus_size']:\n",
    "        print(\"  Skipping download (sufficient papers already downloaded)\")\n",
    "    else:\n",
    "        print(f\"  Need to download {CONFIG['corpus_size'] - n_existing} more papers\")\n",
    "        download_corpus = True\n",
    "else:\n",
    "    print(\"\\nüì• Downloading PubMed corpus...\")\n",
    "    print(f\"   Query: {CONFIG['pubmed_query']}\")\n",
    "    print(f\"   Max papers: {CONFIG['corpus_size']}\")\n",
    "    print(f\"   Date range: {CONFIG['date_range']}\")\n",
    "    print(\"\\n‚è±Ô∏è This will take 30-60 minutes...\")\n",
    "    \n",
    "    download_corpus = True\n",
    "\n",
    "if download_corpus:\n",
    "    # Initialize corpus manager\n",
    "    manager = PubMedCorpusManager(\n",
    "        save_path=CONFIG['corpus_path'],\n",
    "        email=NCBI_EMAIL\n",
    "    )\n",
    "    \n",
    "    # Download\n",
    "    articles = manager.download_pubmed_abstracts(\n",
    "        query=CONFIG['pubmed_query'],\n",
    "        max_results=CONFIG['corpus_size'],\n",
    "        date_range=CONFIG['date_range']\n",
    "    )\n",
    "    \n",
    "    if articles:\n",
    "        # Save corpus\n",
    "        manager.save_corpus(articles)\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = manager.get_corpus_statistics()\n",
    "        print(\"\\nüìä Corpus Statistics:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Downloaded {len(articles)} papers!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Download failed - check your internet and NCBI email\")\n",
    "        raise RuntimeError(\"Corpus download failed\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Using existing corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Checkpoint: Corpus Downloaded\n",
    "\n",
    "At this point, you have:\n",
    "- ‚úÖ PubMed corpus downloaded to Google Drive\n",
    "- ‚úÖ Environment fully configured\n",
    "\n",
    "**If you need to stop here:**\n",
    "- Your corpus is safely stored in Google Drive\n",
    "- You can resume from the next cell later\n",
    "\n",
    "**To continue:** Run the next cells to build the search index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a comprehensive Jupyter notebook, but due to size constraints, I'm providing the first 5 critical cells. The complete notebook would continue with:\n",
    "\n",
    "- Cells 6-7: Build FAISS index\n",
    "- Cells 8-9: Prepare training data\n",
    "- Cells 10-15: Iteration 1 training\n",
    "- Cells 16-21: Iteration 2 training\n",
    "- Cells 22-27: Iteration 3 training\n",
    "- Cells 28-30: Evaluation\n",
    "\n",
    "Would you like me to:\n",
    "1. Continue with the remaining cells in the notebook?\n",
    "2. Create a simplified version?\n",
    "3. Focus on specific sections?\n",
    "\n",
    "Let me know how you'd like to proceed!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}