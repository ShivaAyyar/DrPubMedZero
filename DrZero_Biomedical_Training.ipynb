{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Zero Biomedical Training (Lightweight Version)\n",
    "\n",
    "**Optimized for:** Single Google Drive (<15GB), A100 GPU (<20 hours)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains a lightweight version of Dr. Zero on PubMed biomedical literature:\n",
    "- **Corpus**: 10,000 papers (~2 GB)\n",
    "- **Training**: Single iteration only (~10-15 hours)\n",
    "- **Storage**: <15 GB on single Google Drive\n",
    "- **Checkpoints**: Temp storage + final checkpoint to Drive\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab Pro/Pro+ (A100 GPU)\n",
    "- <15 GB Google Drive storage\n",
    "- Weights & Biases account (optional)\n",
    "- Your email for NCBI PubMed API\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Run Cell 1 (Mount Drive)\n",
    "2. Run Cell 2 (Install Dependencies) - **THEN RESTART RUNTIME**\n",
    "3. Run Cell 1 again (re-mount Drive after restart)\n",
    "4. Skip Cell 2, continue from Cell 3 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Mount Google Drive & Setup Directories\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Mount single Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory structure\n",
    "# Google Drive: Only final models and corpus (~12 GB)\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_lite')\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus'\n",
    "FINAL_CHECKPOINT_DIR = DRIVE_BASE / 'final_checkpoint'\n",
    "\n",
    "# Local temp: Intermediate checkpoints, working files (auto-deleted on disconnect)\n",
    "LOCAL_BASE = Path('/content/drzero_temp')\n",
    "LOCAL_CHECKPOINT = LOCAL_BASE / 'checkpoints'\n",
    "LOCAL_DATA = LOCAL_BASE / 'data'\n",
    "LOCAL_LOGS = LOCAL_BASE / 'logs'\n",
    "\n",
    "# Create all directories\n",
    "for dir_path in [DRIVE_BASE, CORPUS_DIR, FINAL_CHECKPOINT_DIR,\n",
    "                 LOCAL_BASE, LOCAL_CHECKPOINT, LOCAL_DATA, LOCAL_LOGS]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Storage Layout:\")\n",
    "print(f\"  Google Drive (~12 GB): {DRIVE_BASE}\")\n",
    "print(f\"    - Corpus: {CORPUS_DIR}\")\n",
    "print(f\"    - Final model: {FINAL_CHECKPOINT_DIR}\")\n",
    "print(f\"  Local temp (auto-deleted): {LOCAL_BASE}\")\n",
    "print(f\"    - Working checkpoints: {LOCAL_CHECKPOINT}\")\n",
    "print(f\"    - Data: {LOCAL_DATA}\")\n",
    "print(\"\\n[OK] Directories created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 2: Install Dependencies\n# IMPORTANT: After this cell completes, go to Runtime -> Restart session\n# Then skip this cell and continue from Cell 3\n\nimport subprocess\nimport sys\nimport os\n\nprint(\"Installing dependencies...\")\nprint(\"This may take 5-10 minutes.\\n\")\n\n# Upgrade pip first\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"])\n\n# Install numpy first to avoid binary incompatibility\nprint(\"1/6 Installing numpy...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy<2.0\"])\n\n# Core ML packages\nprint(\"2/6 Installing ML packages...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n    \"torch\", \"transformers\", \"accelerate\", \"datasets\", \"sentence-transformers\"])\n\n# Utility packages\nprint(\"3/6 Installing utilities...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"biopython\", \"wandb\", \"tqdm\", \"psutil\", \"faiss-cpu\", \"uvicorn\", \"fastapi\", \"pydantic\", \"pandas\", \"pyarrow\"])\n\n# Install SGLang for model serving\nprint(\"4/6 Installing SGLang...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sglang[all]\"])\n\n# Clone and install veRL\nprint(\"5/6 Installing veRL...\")\nif not os.path.exists('/content/verl'):\n    subprocess.check_call([\"git\", \"clone\", \"-q\", \"https://github.com/volcengine/verl.git\", \"/content/verl\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/verl\"])\n\n# Install repo as package for biomedical modules\nprint(\"6/6 Installing DrPubMedZero...\")\nif os.path.exists('/content/DrPubMedZero'):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"/content/DrPubMedZero\"])\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"IMPORTANT: RESTART RUNTIME NOW\")\nprint(\"=\"*60)\nprint(\"\\n1. Go to: Runtime -> Restart session\")\nprint(\"2. After restart, run Cell 1 again (mount drive)\")\nprint(\"3. Then SKIP this cell and continue from Cell 3\")\nprint(\"\\nThis restart is required for numpy to load correctly.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Clone Repository & Verify Setup\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify numpy loaded correctly\n",
    "print(\"Verifying installations...\")\n",
    "import numpy as np\n",
    "print(f\"  [OK] numpy {np.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"  [OK] torch {torch.__version__}\")\n",
    "assert torch.cuda.is_available(), \"ERROR: No GPU! Go to Runtime -> Change runtime type -> A100 GPU\"\n",
    "print(f\"  [OK] GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"  [OK] transformers {transformers.__version__}\")\n",
    "\n",
    "import faiss\n",
    "print(f\"  [OK] faiss\")\n",
    "\n",
    "# Clone repository\n",
    "REPO_DIR = Path('/content/DrPubMedZero')\n",
    "\n",
    "print(\"\\nSetting up repository...\")\n",
    "if not REPO_DIR.exists():\n",
    "    subprocess.check_call([\n",
    "        \"git\", \"clone\",\n",
    "        \"https://github.com/ShivaAyyar/DrPubMedZero.git\",\n",
    "        str(REPO_DIR)\n",
    "    ])\n",
    "    print(\"  [OK] Repository cloned\")\n",
    "else:\n",
    "    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"pull\"], \n",
    "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print(\"  [OK] Repository updated\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "# Verify biomedical module\n",
    "from biomedical import PubMedCorpusManager\n",
    "print(\"  [OK] Biomedical module loaded\")\n",
    "\n",
    "print(f\"\\n[OK] All checks passed! Repository: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Configuration\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "# Re-establish directory paths (in case of restart)\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/drzero_lite')\n",
    "CORPUS_DIR = DRIVE_BASE / 'corpus'\n",
    "FINAL_CHECKPOINT_DIR = DRIVE_BASE / 'final_checkpoint'\n",
    "LOCAL_BASE = Path('/content/drzero_temp')\n",
    "LOCAL_CHECKPOINT = LOCAL_BASE / 'checkpoints'\n",
    "LOCAL_DATA = LOCAL_BASE / 'data'\n",
    "LOCAL_LOGS = LOCAL_BASE / 'logs'\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [CORPUS_DIR, FINAL_CHECKPOINT_DIR, LOCAL_CHECKPOINT, LOCAL_DATA, LOCAL_LOGS]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "    \n",
    "    # Data - Lightweight settings\n",
    "    'corpus_size': 10000,\n",
    "    'training_seeds': 500,\n",
    "    'pubmed_query': '(breast cancer OR lung cancer) AND (gene OR protein)',\n",
    "    'date_range': ('2022/01/01', '2024/12/31'),\n",
    "    \n",
    "    # Training - Lightweight settings\n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation': 4,\n",
    "    'learning_rate': 1e-6,\n",
    "    'max_steps': 100,\n",
    "    'save_freq': 50,\n",
    "    \n",
    "    # Paths\n",
    "    'corpus_dir': str(CORPUS_DIR),\n",
    "    'corpus_file': str(CORPUS_DIR / 'pubmed-corpus.jsonl'),\n",
    "    'index_file': str(CORPUS_DIR / 'pubmedbert_index.faiss'),\n",
    "    'final_checkpoint': str(FINAL_CHECKPOINT_DIR),\n",
    "    'temp_checkpoint': str(LOCAL_CHECKPOINT),\n",
    "    'data_dir': str(LOCAL_DATA),\n",
    "    'logs_dir': str(LOCAL_LOGS),\n",
    "    \n",
    "    # Server ports\n",
    "    'retrieval_port': 8000,\n",
    "    'solver_port': 8001,\n",
    "}\n",
    "\n",
    "# User email for NCBI\n",
    "CONFIG['email'] = 'ssa163@case.edu'\n",
    "print(f\"NCBI Email: {CONFIG['email']}\")\n",
    "\n",
    "# Weights & Biases (optional)\n",
    "wandb_key = getpass.getpass(\"W&B API key (press Enter to skip): \")\n",
    "if wandb_key.strip():\n",
    "    os.environ['WANDB_API_KEY'] = wandb_key\n",
    "    import wandb\n",
    "    wandb.login(key=wandb_key)\n",
    "    print(\"  [OK] W&B configured\")\n",
    "else:\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "    print(\"  [OK] W&B disabled\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Corpus: {CONFIG['corpus_size']} papers\")\n",
    "print(f\"  Training: {CONFIG['max_steps']} steps\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"\\n[OK] Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Download PubMed Corpus\n",
    "\n",
    "from biomedical import PubMedCorpusManager\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "\n",
    "# Check if corpus already exists\n",
    "if corpus_file.exists():\n",
    "    with open(corpus_file) as f:\n",
    "        n_papers = sum(1 for _ in f)\n",
    "    print(f\"Corpus already exists: {n_papers} papers\")\n",
    "    \n",
    "    if n_papers >= CONFIG['corpus_size']:\n",
    "        print(\"Skipping download.\")\n",
    "        download_needed = False\n",
    "    else:\n",
    "        print(f\"Need {CONFIG['corpus_size'] - n_papers} more papers\")\n",
    "        download_needed = True\n",
    "else:\n",
    "    print(f\"Downloading {CONFIG['corpus_size']} PubMed papers...\")\n",
    "    print(\"Estimated time: 15-30 minutes\\n\")\n",
    "    download_needed = True\n",
    "\n",
    "if download_needed:\n",
    "    manager = PubMedCorpusManager(\n",
    "        save_path=CONFIG['corpus_dir'],\n",
    "        email=CONFIG['email']\n",
    "    )\n",
    "    \n",
    "    articles = manager.download_pubmed_abstracts(\n",
    "        query=CONFIG['pubmed_query'],\n",
    "        max_results=CONFIG['corpus_size'],\n",
    "        date_range=CONFIG['date_range']\n",
    "    )\n",
    "    \n",
    "    if articles:\n",
    "        manager.save_corpus(articles)\n",
    "        print(f\"\\n[OK] Downloaded {len(articles)} papers!\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Download failed - check internet connection and email\")\n",
    "else:\n",
    "    print(\"[OK] Using existing corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Build FAISS Index\n",
    "\n",
    "from biomedical.biomedical_retriever import BiomedicalRetrieverServer\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "index_file = Path(CONFIG['index_file'])\n",
    "\n",
    "if index_file.exists():\n",
    "    print(f\"Index already exists: {index_file}\")\n",
    "    print(\"Skipping index build.\")\n",
    "else:\n",
    "    print(\"Building PubMedBERT FAISS index...\")\n",
    "    print(\"Estimated time: 10-20 minutes\\n\")\n",
    "    \n",
    "    # The BiomedicalRetrieverServer will build and save the index\n",
    "    retriever = BiomedicalRetrieverServer(\n",
    "        corpus_path=str(corpus_file),\n",
    "        index_path=str(index_file),\n",
    "        model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        device=\"cuda\",\n",
    "        topk=3\n",
    "    )\n",
    "    \n",
    "    # Test search\n",
    "    print(\"\\nTesting search...\")\n",
    "    results = retriever.search(\"TP53 mutation breast cancer\")\n",
    "    print(f\"  Found {len(results)} results\")\n",
    "    \n",
    "    # Clean up to free GPU memory\n",
    "    del retriever\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    size_mb = os.path.getsize(index_file) / (1024**2)\n",
    "    print(f\"\\n[OK] Index built: {size_mb:.0f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Prepare Training Seeds\n",
    "\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "corpus_file = Path(CONFIG['corpus_file'])\n",
    "seeds_file = Path(CONFIG['data_dir']) / 'seeds.jsonl'\n",
    "\n",
    "if seeds_file.exists():\n",
    "    with open(seeds_file) as f:\n",
    "        n_seeds = sum(1 for _ in f)\n",
    "    print(f\"Seeds already exist: {n_seeds}\")\n",
    "    if n_seeds >= CONFIG['training_seeds']:\n",
    "        print(\"Skipping seed preparation.\")\n",
    "        prepare_needed = False\n",
    "    else:\n",
    "        prepare_needed = True\n",
    "else:\n",
    "    prepare_needed = True\n",
    "\n",
    "if prepare_needed:\n",
    "    print(f\"Preparing {CONFIG['training_seeds']} training seeds...\")\n",
    "    \n",
    "    # Load corpus\n",
    "    corpus = []\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    print(f\"  Loaded {len(corpus)} papers from corpus\")\n",
    "    \n",
    "    # Filter for papers with substantial abstracts\n",
    "    substantial = [p for p in corpus if len(p.get('abstract', '').split()) > 100]\n",
    "    print(f\"  {len(substantial)} papers have >100 word abstracts\")\n",
    "    \n",
    "    # Sample seeds\n",
    "    source = substantial if len(substantial) >= CONFIG['training_seeds'] else corpus\n",
    "    seeds = random.sample(source, min(CONFIG['training_seeds'], len(source)))\n",
    "    \n",
    "    # Save seeds\n",
    "    with open(seeds_file, 'w') as f:\n",
    "        for seed in seeds:\n",
    "            f.write(json.dumps(seed) + '\\n')\n",
    "    \n",
    "    print(f\"\\n[OK] Saved {len(seeds)} training seeds\")\n",
    "else:\n",
    "    print(\"[OK] Using existing seeds\")\n",
    "\n",
    "CONFIG['seeds_file'] = str(seeds_file)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 7.5: Convert Seeds to Parquet Format for veRL\n\nimport json\nimport pandas as pd\nfrom pathlib import Path\n\nprint(\"=\"*60)\nprint(\"CONVERTING SEEDS TO PARQUET FORMAT\")\nprint(\"=\"*60)\n\nseeds_file = Path(CONFIG['seeds_file'])\nparquet_file = Path(CONFIG['data_dir']) / 'training_seeds.parquet'\n\nif parquet_file.exists():\n    print(f\"\\nParquet file already exists: {parquet_file}\")\n    df = pd.read_parquet(parquet_file)\n    print(f\"  Contains {len(df)} examples\")\n    CONFIG['train_parquet'] = str(parquet_file)\nelse:\n    print(\"\\nConverting JSONL seeds to parquet...\")\n    \n    # Load seeds\n    with open(seeds_file) as f:\n        seeds = [json.loads(line) for line in f]\n    print(f\"  Loaded {len(seeds)} seeds\")\n    \n    # Convert to veRL format\n    # veRL expects: prompt (list of messages), data_source, extra_info\n    data = []\n    for idx, seed in enumerate(seeds):\n        title = seed.get('title', '')\n        abstract = seed.get('abstract', seed.get('text', ''))[:500]  # Truncate for memory\n        pmid = seed.get('pmid', str(idx))\n        \n        # Create prompt for proposer to generate QA pair\n        user_message = f\"Generate a challenging biomedical question-answer pair from this paper:\\n\\nTitle: {title}\\n\\nAbstract: {abstract}\"\n        \n        data.append({\n            'prompt': [\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            'data_source': 'search_biomedical_1',  # 1-hop reasoning for iteration 1\n            'extra_info': {\n                'index': idx,\n                'pmid': pmid,\n                'tools_kwargs': {},\n                'interaction_kwargs': {}\n            }\n        })\n    \n    # Save as parquet\n    df = pd.DataFrame(data)\n    df.to_parquet(parquet_file)\n    \n    print(f\"  Created {len(df)} training examples\")\n    print(f\"  Saved to: {parquet_file}\")\n    \n    CONFIG['train_parquet'] = str(parquet_file)\n\nprint(f\"\\n[OK] Training data ready: {CONFIG['train_parquet']}\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: Train Proposer with veRL GRPO + Biomedical Reward Function\n\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nimport pandas as pd\n\nREPO_DIR = Path('/content/DrPubMedZero')\nos.chdir(REPO_DIR)\n\n# Ensure repo is in PYTHONPATH for reward function imports\nsys.path.insert(0, str(REPO_DIR))\n\nprint(\"=\"*60)\nprint(\"DR. ZERO BIOMEDICAL PROPOSER TRAINING\")\nprint(\"Using GRPO + compute_biomedical_challenger_score_batch\")\nprint(\"=\"*60)\n\n# Calculate dataset size for step estimation\ndf = pd.read_parquet(CONFIG['train_parquet'])\nestimated_steps = len(df) // CONFIG['batch_size']\n\nprint(f\"\\nTraining Configuration:\")\nprint(f\"  Algorithm: GRPO (Group Relative Policy Optimization)\")\nprint(f\"  Reward: compute_biomedical_challenger_score_batch (paper's function)\")\nprint(f\"  Model: {CONFIG['model_name']}\")\nprint(f\"  Dataset: {len(df)} examples\")\nprint(f\"  Batch size: {CONFIG['batch_size']}\")\nprint(f\"  Estimated steps: {estimated_steps}\")\nprint(f\"  Solver rollout: 3 attempts per question\")\nprint(f\"  Checkpoints: Every 25 steps\")\nprint(f\"  Storage: Temp checkpoints → {CONFIG['temp_checkpoint']}\")\nprint(f\"           Final checkpoint → {CONFIG['final_checkpoint']}\")\nprint()\n\n# Training command (from iter1_challenger_biomed.sh, adapted for 1 GPU + 80GB RAM)\ntrain_cmd = [\n    sys.executable, \"-m\", \"verl.trainer.main_ppo\",\n    \"--config-path\", str(REPO_DIR / \"config\"),\n    \"--config-name\", \"search_multiturn_grpo\",\n\n    # Data\n    f\"data.train_files={CONFIG['train_parquet']}\",\n    f\"data.train_batch_size={CONFIG['batch_size']}\",\n\n    # Model\n    f\"actor_rollout_ref.model.path={CONFIG['model_name']}\",\n    f\"actor_rollout_ref.actor.optim.lr={CONFIG['learning_rate']}\",\n    \"actor_rollout_ref.actor.grad_clip=0.1\",\n    \"actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.03\",\n\n    # GRPO algorithm (PAPER'S ALGORITHM!)\n    \"algorithm.use_kl_in_reward=False\",\n    \"algorithm.adv_estimator=grpo_batch\",\n    \"actor_rollout_ref.rollout.n=1\",  # GRPO group size\n    \"actor_rollout_ref.actor.use_kl_loss=False\",\n\n    # Reward function (PAPER'S REWARD FUNCTION!)\n    \"reward_model.reward_manager=batch\",\n    \"custom_reward_function.name=compute_biomedical_challenger_score_batch\",\n    \"custom_reward_function.path=verl/custom_reward/reward_function.py\",\n    f\"custom_reward_function.reward_kwargs.model_name={CONFIG['model_name']}\",\n    \"custom_reward_function.reward_kwargs.base_url=http://127.0.0.1:8001\",\n    \"custom_reward_function.reward_kwargs.reward_rollout_n=3\",  # Solver attempts\n\n    # Single GPU settings (HARDWARE ADAPTATION ONLY)\n    \"trainer.n_gpus_per_node=1\",\n    \"trainer.nnodes=1\",\n    \"actor_rollout_ref.rollout.tensor_model_parallel_size=1\",\n    \"actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2\",\n    \"actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=2\",\n    \"actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=2\",\n\n    # Memory optimization for 80GB A100\n    \"actor_rollout_ref.actor.fsdp_config.param_offload=True\",\n    \"actor_rollout_ref.actor.fsdp_config.optimizer_offload=True\",\n    \"actor_rollout_ref.model.enable_gradient_checkpointing=True\",\n    \"actor_rollout_ref.model.use_remove_padding=True\",\n    \"actor_rollout_ref.rollout.gpu_memory_utilization=0.35\",  # Matches solver allocation\n\n    # Multi-turn tool use (PAPER'S APPROACH!)\n    f\"actor_rollout_ref.rollout.multi_turn.tool_config_path={str(REPO_DIR / 'config' / 'search_tool_config.yaml')}\",\n\n    # Training schedule\n    \"trainer.total_epochs=1\",\n    f\"trainer.save_freq={CONFIG['save_freq']}\",\n    f\"trainer.default_hdfs_dir={CONFIG['temp_checkpoint']}\",\n    \"trainer.project_name=drzero-biomed-poc\",\n    \"trainer.experiment_name=iter1_biomedical_colab\",\n    \"trainer.logger=[\\\"console\\\"]\",\n    \"trainer.val_before_train=False\",\n    \"trainer.test_freq=-1\",\n]\n\nprint(\"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\nprint(\"\\nExpected output:\")\nprint(\"  - Format scores (XML structure, tool calls)\")\nprint(\"  - Solver responses (3 attempts per question)\")\nprint(\"  - Difficulty scores (how much solvers vary)\")\nprint(\"  - Final rewards (format + difficulty)\")\nprint()\nprint(\"This will take 1-2 hours for ~15 steps on A100 80GB\")\nprint(\"=\"*60 + \"\\n\")\n\n# Run training\ntry:\n    process = subprocess.run(train_cmd, cwd=str(REPO_DIR))\n    \n    if process.returncode == 0:\n        print(\"\\n\" + \"=\"*60)\n        print(\"✓ TRAINING COMPLETE\")\n        print(\"=\"*60)\n        \n        # Copy final checkpoint to Drive (only final, save space)\n        import shutil\n        checkpoints = sorted(Path(CONFIG['temp_checkpoint']).glob(\"global_step_*\"))\n        if checkpoints:\n            final_ckpt = checkpoints[-1]\n            dest = Path(CONFIG['final_checkpoint']) / final_ckpt.name\n            print(f\"\\nCopying final checkpoint to Google Drive...\")\n            print(f\"  From: {final_ckpt}\")\n            print(f\"  To: {dest}\")\n            \n            if dest.exists():\n                shutil.rmtree(dest)\n            shutil.copytree(final_ckpt, dest)\n            \n            # Get size\n            size_gb = sum(f.stat().st_size for f in dest.rglob('*') if f.is_file()) / 1e9\n            print(f\"  Size: {size_gb:.2f} GB\")\n            print(f\"\\n[OK] Final checkpoint saved to Drive\")\n        else:\n            print(\"\\n[WARNING] No checkpoints found\")\n    else:\n        print(f\"\\n\" + \"=\"*60)\n        print(f\"❌ TRAINING FAILED (exit code: {process.returncode})\")\n        print(\"=\"*60)\n        print(\"\\nCheck for errors above. Common issues:\")\n        print(\"  1. Servers not running (run Cell 8.5 first)\")\n        print(\"  2. Out of memory (reduce batch_size in Cell 4)\")\n        print(\"  3. Import errors (restart runtime after Cell 2)\")\n        \nexcept KeyboardInterrupt:\n    print(\"\\n\\nTraining interrupted by user.\")\n    print(f\"Partial checkpoints saved to: {CONFIG['temp_checkpoint']}\")\nexcept Exception as e:\n    print(f\"\\n\\nTraining failed with exception: {e}\")\n    import traceback\n    traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 9.5: Shutdown Servers and Cleanup\n\nimport subprocess\nimport os\nimport signal\n\nprint(\"=\"*60)\nprint(\"SHUTTING DOWN SERVERS\")\nprint(\"=\"*60)\n\n# Close log files first\nif 'retrieval_log' in CONFIG and CONFIG['retrieval_log']:\n    try:\n        CONFIG['retrieval_log'].close()\n    except:\n        pass\n\nif 'solver_log' in CONFIG and CONFIG['solver_log']:\n    try:\n        CONFIG['solver_log'].close()\n    except:\n        pass\n\n# Kill processes\nservers_stopped = []\n\nif 'retrieval_pid' in CONFIG:\n    try:\n        os.kill(CONFIG['retrieval_pid'], signal.SIGTERM)\n        servers_stopped.append(f\"Retrieval server (PID: {CONFIG['retrieval_pid']})\")\n    except:\n        pass\n\nif 'solver_pid' in CONFIG:\n    try:\n        os.kill(CONFIG['solver_pid'], signal.SIGTERM)\n        servers_stopped.append(f\"Solver server (PID: {CONFIG['solver_pid']})\")\n    except:\n        pass\n\n# Force kill any remaining processes on these ports\nsubprocess.run(\"fuser -k 8000/tcp 2>/dev/null || lsof -ti:8000 | xargs kill -9 2>/dev/null || true\", shell=True)\nsubprocess.run(\"fuser -k 8001/tcp 2>/dev/null || lsof -ti:8001 | xargs kill -9 2>/dev/null || true\", shell=True)\n\nif servers_stopped:\n    for server in servers_stopped:\n        print(f\"  ✓ Stopped {server}\")\nelse:\n    print(\"  No servers were running\")\n\n# Show temp checkpoint usage\nfrom pathlib import Path\ntemp_size = 0\ntemp_path = Path(CONFIG['temp_checkpoint'])\nif temp_path.exists():\n    temp_size = sum(f.stat().st_size for f in temp_path.rglob('*') if f.is_file()) / 1e9\n    print(f\"\\nTemp checkpoint storage: {temp_size:.2f} GB\")\n    print(f\"  Location: {temp_path}\")\n    print(f\"  (Will be auto-deleted when Colab session ends)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"[OK] Cleanup complete\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Test Retrieval & Model Loading\n",
    "# This cell tests that everything works before starting the long training\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRE-TRAINING VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Load and test retriever\n",
    "print(\"\\n1. Testing retriever...\")\n",
    "from biomedical.biomedical_retriever import BiomedicalRetrieverServer\n",
    "import torch\n",
    "\n",
    "retriever = BiomedicalRetrieverServer(\n",
    "    corpus_path=CONFIG['corpus_file'],\n",
    "    index_path=CONFIG['index_file'],\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "results = retriever.search(\"breast cancer chemotherapy resistance\")\n",
    "print(f\"   [OK] Retriever working - found {len(results)} results\")\n",
    "if results:\n",
    "    print(f\"   Sample: {results[0].get('title', 'N/A')[:60]}...\")\n",
    "\n",
    "# Clean up retriever\n",
    "del retriever\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Test 2: Load model\n",
    "print(\"\\n2. Testing model loading...\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"   Loading {CONFIG['model_name']}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Quick generation test\n",
    "inputs = tokenizer(\"What is breast cancer?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"   [OK] Model generating: {response[:50]}...\")\n",
    "\n",
    "# Clean up\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Test 3: Check GPU memory\n",
    "print(\"\\n3. GPU Memory Status:\")\n",
    "total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "used = torch.cuda.memory_allocated() / 1e9\n",
    "free = total - used\n",
    "print(f\"   Total: {total:.1f} GB\")\n",
    "print(f\"   Used: {used:.1f} GB\")\n",
    "print(f\"   Free: {free:.1f} GB\")\n",
    "\n",
    "# Test 4: Check files\n",
    "print(\"\\n4. Required Files:\")\n",
    "from pathlib import Path\n",
    "files_ok = True\n",
    "for name, path in [('Corpus', CONFIG['corpus_file']), \n",
    "                   ('Index', CONFIG['index_file']),\n",
    "                   ('Seeds', CONFIG['seeds_file'])]:\n",
    "    exists = Path(path).exists()\n",
    "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
    "    print(f\"   {status} {name}: {path}\")\n",
    "    if not exists:\n",
    "        files_ok = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if files_ok:\n",
    "    print(\"ALL CHECKS PASSED - Ready for training!\")\n",
    "    print(\"\\nNext: Run Cell 9 to start training.\")\n",
    "    print(\"Training will take approximately 10-15 hours on A100.\")\n",
    "else:\n",
    "    print(\"SOME CHECKS FAILED - Please fix issues above before training.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 9: Train Biomedical Proposer (Simplified for Colab)\n# Uses HuggingFace TRL instead of veRL for single-GPU compatibility\n\nimport os\nimport sys\nimport json\nimport torch\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nfrom datasets import Dataset\n\nprint(\"=\"*60)\nprint(\"BIOMEDICAL PROPOSER TRAINING\")\nprint(\"=\"*60)\n\n# Verify GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# === STEP 1: Create Training Data ===\nprint(\"\\n[1/4] Preparing training data...\")\n\n# Load seeds and create biomedical QA training prompts\nseeds_file = Path(CONFIG['seeds_file'])\ncorpus_file = Path(CONFIG['corpus_file'])\n\ntraining_examples = []\nwith open(seeds_file) as f:\n    seeds = [json.loads(line) for line in f]\n\nprint(f\"  Loaded {len(seeds)} seed papers\")\n\n# Create multi-hop biomedical question prompts\n# The proposer learns to generate research questions from paper contexts\nsystem_prompt = \"\"\"You are a biomedical research assistant. Given a scientific paper abstract, generate a challenging multi-hop research question that would require searching PubMed to answer. The question should connect multiple biomedical concepts and require reasoning across papers.\n\nFormat your response as a single research question.\"\"\"\n\nfor i, seed in enumerate(seeds):\n    abstract = seed.get('abstract', seed.get('text', ''))[:1000]  # Truncate long abstracts\n    title = seed.get('title', '')\n    \n    # Create training example: paper context -> research question\n    prompt = f\"Based on this paper about '{title}':\\n\\n{abstract}\\n\\nGenerate a research question:\"\n    \n    # For initial training, we use simple self-supervision\n    # The model learns the format of good biomedical questions\n    training_examples.append({\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        \"pmid\": seed.get('pmid', str(i))\n    })\n\nprint(f\"  Created {len(training_examples)} training examples\")\n\n# === STEP 2: Load Model and Tokenizer ===\nprint(\"\\n[2/4] Loading model...\")\n\nmodel_name = CONFIG['model_name']\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    use_cache=False  # Required for gradient checkpointing\n)\nmodel.gradient_checkpointing_enable()\n\nprint(f\"  Model loaded: {model_name}\")\nprint(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n\n# === STEP 3: Prepare Dataset ===\nprint(\"\\n[3/4] Tokenizing dataset...\")\n\ndef tokenize_function(examples):\n    # Apply chat template\n    texts = []\n    for msgs in examples[\"messages\"]:\n        text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n        texts.append(text)\n    \n    tokenized = tokenizer(\n        texts,\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n    return tokenized\n\ndataset = Dataset.from_list(training_examples)\ndataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=32,\n    remove_columns=[\"messages\", \"pmid\"],\n    desc=\"Tokenizing\"\n)\ndataset.set_format(\"torch\")\n\nprint(f\"  Dataset size: {len(dataset)}\")\n\n# === STEP 4: Training ===\nprint(\"\\n[4/4] Starting training...\")\nprint(f\"  Steps: {CONFIG['max_steps']}\")\nprint(f\"  Batch size: {CONFIG['batch_size']}\")\nprint(f\"  Learning rate: {CONFIG['learning_rate']}\")\n\n# Simple training loop for Colab (avoids TRL/veRL complexity)\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# Dataloader\ntrain_dataloader = DataLoader(\n    dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    drop_last=True\n)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n\n# Scheduler\nnum_training_steps = min(CONFIG['max_steps'], len(train_dataloader) * CONFIG['gradient_accumulation'])\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.03 * num_training_steps),\n    num_training_steps=num_training_steps\n)\n\n# Training loop\nmodel.train()\nglobal_step = 0\ntotal_loss = 0\ncheckpoint_dir = Path(CONFIG['temp_checkpoint'])\n\nprogress_bar = tqdm(total=CONFIG['max_steps'], desc=\"Training\")\n\nfor epoch in range(10):  # Max epochs (will break early based on steps)\n    for batch_idx, batch in enumerate(train_dataloader):\n        # Move to GPU\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss / CONFIG['gradient_accumulation']\n        \n        # Backward pass\n        loss.backward()\n        total_loss += loss.item()\n        \n        # Update weights every gradient_accumulation steps\n        if (batch_idx + 1) % CONFIG['gradient_accumulation'] == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n            global_step += 1\n            avg_loss = total_loss * CONFIG['gradient_accumulation']\n            total_loss = 0\n            \n            progress_bar.update(1)\n            progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n            \n            # Save checkpoint\n            if global_step % CONFIG['save_freq'] == 0:\n                ckpt_path = checkpoint_dir / f\"step_{global_step}\"\n                ckpt_path.mkdir(parents=True, exist_ok=True)\n                model.save_pretrained(ckpt_path)\n                tokenizer.save_pretrained(ckpt_path)\n                print(f\"\\n  Saved checkpoint: {ckpt_path}\")\n            \n            if global_step >= CONFIG['max_steps']:\n                break\n    \n    if global_step >= CONFIG['max_steps']:\n        break\n\nprogress_bar.close()\n\n# === STEP 5: Save Final Model ===\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)\n\n# Save to Google Drive\nfinal_path = Path(CONFIG['final_checkpoint']) / \"proposer_final\"\nfinal_path.mkdir(parents=True, exist_ok=True)\nmodel.save_pretrained(final_path)\ntokenizer.save_pretrained(final_path)\n\nprint(f\"\\nFinal model saved to: {final_path}\")\nprint(f\"Total steps: {global_step}\")\n\n# Free memory\ndel model, optimizer\ntorch.cuda.empty_cache()\nprint(\"\\n[OK] Training complete! Model saved to Google Drive.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 10: Check Storage & Results\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    path = Path(path)\n",
    "    if path.exists():\n",
    "        for f in path.rglob('*'):\n",
    "            if f.is_file():\n",
    "                total += f.stat().st_size\n",
    "    return total / (1024**3)  # GB\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STORAGE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "drive_base = Path('/content/drive/MyDrive/drzero_lite')\n",
    "local_base = Path('/content/drzero_temp')\n",
    "\n",
    "print(\"\\nGoogle Drive (persistent):\")\n",
    "corpus_size = get_dir_size(drive_base / 'corpus')\n",
    "ckpt_size = get_dir_size(drive_base / 'final_checkpoint')\n",
    "print(f\"  Corpus: {corpus_size:.2f} GB\")\n",
    "print(f\"  Final checkpoint: {ckpt_size:.2f} GB\")\n",
    "print(f\"  TOTAL: {corpus_size + ckpt_size:.2f} GB / 15 GB\")\n",
    "\n",
    "print(\"\\nLocal temp (will be deleted):\")\n",
    "temp_size = get_dir_size(local_base)\n",
    "print(f\"  Temp files: {temp_size:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "total_drive = corpus_size + ckpt_size\n",
    "if total_drive < 15:\n",
    "    print(f\"[OK] Within 15 GB Drive limit ({total_drive:.1f} GB used)\")\n",
    "else:\n",
    "    print(f\"[WARNING] Exceeds 15 GB limit ({total_drive:.1f} GB used)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# List checkpoints\n",
    "print(\"\\nSaved checkpoints:\")\n",
    "ckpt_dir = drive_base / 'final_checkpoint'\n",
    "if ckpt_dir.exists():\n",
    "    for item in sorted(ckpt_dir.iterdir()):\n",
    "        print(f\"  {item.name}\")\n",
    "else:\n",
    "    print(\"  None yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Complete!\n",
    "\n",
    "## What You Have\n",
    "\n",
    "- PubMed corpus (10K papers)\n",
    "- PubMedBERT search index\n",
    "- Trained proposer model\n",
    "- All saved to Google Drive (<15 GB)\n",
    "\n",
    "## Download Your Model\n",
    "\n",
    "```python\n",
    "# Zip and download\n",
    "!zip -r model.zip /content/drive/MyDrive/drzero_lite/final_checkpoint\n",
    "from google.colab import files\n",
    "files.download('model.zip')\n",
    "```\n",
    "\n",
    "## Test Your Model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"/content/drive/MyDrive/drzero_lite/final_checkpoint/step_100\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "# Generate\n",
    "prompt = \"Generate a research question about breast cancer and TP53:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Evaluate on PubMedQA benchmark\n",
    "2. Generate biomedical QA pairs\n",
    "3. Fine-tune solver (optional - adds ~5 hours)\n",
    "4. Iterate for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}